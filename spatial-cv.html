<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Geocomputation with R</title>
  <meta name="description" content="Forthcoming book on geographic data with R.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Geocomputation with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://geocompr.robinlovelace.net" />
  
  <meta property="og:description" content="Forthcoming book on geographic data with R." />
  <meta name="github-repo" content="Robinlovelace/geocompr" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Geocomputation with R" />
  
  <meta name="twitter:description" content="Forthcoming book on geographic data with R." />
  

<meta name="author" content="Robin Lovelace, Jakub Nowosad, Jannes Muenchow">


<meta name="date" content="2018-03-27">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="algorithms-and-functions-for-geocomputation.html">
<link rel="next" href="references.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.0/htmlwidgets.js"></script>
<link href="libs/leaflet-0.7.7/leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-0.7.7/leaflet.js"></script>
<link href="libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet" />
<link href="libs/leaflet-label-0.2.2/leaflet.label.css" rel="stylesheet" />
<script src="libs/leaflet-label-0.2.2/leaflet.label.js"></script>
<script src="libs/Proj4Leaflet-0.7.2/proj4-compressed.js"></script>
<script src="libs/Proj4Leaflet-0.7.2/proj4leaflet.js"></script>
<script src="libs/leaflet-binding-1.1.0/leaflet.js"></script>
<script src="libs/leaflet-providers-1.0.27/leaflet-providers.js"></script>
<script src="libs/leaflet-providers-plugin-1.1.0/leaflet-providers-plugin.js"></script>
<link href="libs/leaflet-awesomemarkers-2.0.3/leaflet.awesome-markers.css" rel="stylesheet" />
<script src="libs/leaflet-awesomemarkers-2.0.3/leaflet.awesome-markers.min.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-99618359-1', 'auto');
  ga('send', 'pageview');

</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Geocomputation with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#development"><i class="fa fa-check"></i>Development</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-contribute"><i class="fa fa-check"></i>How to contribute?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#reproducibility"><i class="fa fa-check"></i>Reproducibility</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#what-is-geocomputation"><i class="fa fa-check"></i><b>1.1</b> What is geocomputation?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#why-geocomputation-with-r"><i class="fa fa-check"></i><b>1.2</b> Why Geocomputation with R?</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#software-for-geocomputation"><i class="fa fa-check"></i><b>1.3</b> Software for geocomputation</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#rs-spatial-ecosystem"><i class="fa fa-check"></i><b>1.4</b> R’s spatial ecosystem</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#the-history-of-r-spatial"><i class="fa fa-check"></i><b>1.5</b> The history of R-spatial</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#exercises"><i class="fa fa-check"></i><b>1.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>I Basic methods</b></span></li>
<li class="chapter" data-level="2" data-path="spatial-class.html"><a href="spatial-class.html"><i class="fa fa-check"></i><b>2</b> Geographic data in R</a><ul>
<li class="chapter" data-level="" data-path="spatial-class.html"><a href="spatial-class.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="2.1" data-path="spatial-class.html"><a href="spatial-class.html#vector-data"><i class="fa fa-check"></i><b>2.1</b> Vector data</a><ul>
<li class="chapter" data-level="2.1.1" data-path="spatial-class.html"><a href="spatial-class.html#intro-sf"><i class="fa fa-check"></i><b>2.1.1</b> An introduction to simple features</a></li>
<li class="chapter" data-level="2.1.2" data-path="spatial-class.html"><a href="spatial-class.html#why-simple-features"><i class="fa fa-check"></i><b>2.1.2</b> Why simple features?</a></li>
<li class="chapter" data-level="2.1.3" data-path="spatial-class.html"><a href="spatial-class.html#basic-map"><i class="fa fa-check"></i><b>2.1.3</b> Basic map making</a></li>
<li class="chapter" data-level="2.1.4" data-path="spatial-class.html"><a href="spatial-class.html#base-args"><i class="fa fa-check"></i><b>2.1.4</b> Base plot arguments</a></li>
<li class="chapter" data-level="2.1.5" data-path="spatial-class.html"><a href="spatial-class.html#sf-classes"><i class="fa fa-check"></i><b>2.1.5</b> Simple feature classes</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="spatial-class.html"><a href="spatial-class.html#raster-data"><i class="fa fa-check"></i><b>2.2</b> Raster data</a><ul>
<li class="chapter" data-level="2.2.1" data-path="spatial-class.html"><a href="spatial-class.html#an-introduction-to-raster"><i class="fa fa-check"></i><b>2.2.1</b> An introduction to raster</a></li>
<li class="chapter" data-level="2.2.2" data-path="spatial-class.html"><a href="spatial-class.html#basic-map-raster"><i class="fa fa-check"></i><b>2.2.2</b> Basic map making</a></li>
<li class="chapter" data-level="2.2.3" data-path="spatial-class.html"><a href="spatial-class.html#raster-classes"><i class="fa fa-check"></i><b>2.2.3</b> Raster classes</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="spatial-class.html"><a href="spatial-class.html#crs-intro"><i class="fa fa-check"></i><b>2.3</b> Coordinate Reference Systems</a><ul>
<li class="chapter" data-level="2.3.1" data-path="spatial-class.html"><a href="spatial-class.html#geographic-coordinate-systems"><i class="fa fa-check"></i><b>2.3.1</b> Geographic coordinate systems</a></li>
<li class="chapter" data-level="2.3.2" data-path="spatial-class.html"><a href="spatial-class.html#projected-coordinate-systems"><i class="fa fa-check"></i><b>2.3.2</b> Projected coordinate systems</a></li>
<li class="chapter" data-level="2.3.3" data-path="spatial-class.html"><a href="spatial-class.html#crs-in-r"><i class="fa fa-check"></i><b>2.3.3</b> CRSs in R</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="spatial-class.html"><a href="spatial-class.html#units"><i class="fa fa-check"></i><b>2.4</b> Units</a></li>
<li class="chapter" data-level="2.5" data-path="spatial-class.html"><a href="spatial-class.html#ex2"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="attr.html"><a href="attr.html"><i class="fa fa-check"></i><b>3</b> Attribute operations</a><ul>
<li class="chapter" data-level="" data-path="attr.html"><a href="attr.html#prerequisites-1"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="3.1" data-path="attr.html"><a href="attr.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="attr.html"><a href="attr.html#vector-attribute-manipulation"><i class="fa fa-check"></i><b>3.2</b> Vector attribute manipulation</a><ul>
<li class="chapter" data-level="3.2.1" data-path="attr.html"><a href="attr.html#vector-attribute-subsetting"><i class="fa fa-check"></i><b>3.2.1</b> Vector attribute subsetting</a></li>
<li class="chapter" data-level="3.2.2" data-path="attr.html"><a href="attr.html#vector-attribute-aggregation"><i class="fa fa-check"></i><b>3.2.2</b> Vector attribute aggregation</a></li>
<li class="chapter" data-level="3.2.3" data-path="attr.html"><a href="attr.html#vector-attribute-joining"><i class="fa fa-check"></i><b>3.2.3</b> Vector attribute joining</a></li>
<li class="chapter" data-level="3.2.4" data-path="attr.html"><a href="attr.html#vec-attr-creation"><i class="fa fa-check"></i><b>3.2.4</b> Creating attributes and removing spatial information</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="attr.html"><a href="attr.html#manipulating-raster-objects"><i class="fa fa-check"></i><b>3.3</b> Manipulating raster objects</a><ul>
<li class="chapter" data-level="3.3.1" data-path="attr.html"><a href="attr.html#raster-subsetting"><i class="fa fa-check"></i><b>3.3.1</b> Raster subsetting</a></li>
<li class="chapter" data-level="3.3.2" data-path="attr.html"><a href="attr.html#summarizing-raster-objects"><i class="fa fa-check"></i><b>3.3.2</b> Summarizing raster objects</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="attr.html"><a href="attr.html#exercises-1"><i class="fa fa-check"></i><b>3.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="spatial-operations.html"><a href="spatial-operations.html"><i class="fa fa-check"></i><b>4</b> Spatial operations</a><ul>
<li class="chapter" data-level="" data-path="spatial-operations.html"><a href="spatial-operations.html#prerequisites-2"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="4.1" data-path="spatial-operations.html"><a href="spatial-operations.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="spatial-operations.html"><a href="spatial-operations.html#spatial-vec"><i class="fa fa-check"></i><b>4.2</b> Spatial operations on vector data</a><ul>
<li class="chapter" data-level="4.2.1" data-path="spatial-operations.html"><a href="spatial-operations.html#spatial-subsetting"><i class="fa fa-check"></i><b>4.2.1</b> Spatial subsetting</a></li>
<li class="chapter" data-level="4.2.2" data-path="spatial-operations.html"><a href="spatial-operations.html#topological-relations"><i class="fa fa-check"></i><b>4.2.2</b> Topological relations</a></li>
<li class="chapter" data-level="4.2.3" data-path="spatial-operations.html"><a href="spatial-operations.html#spatial-joining"><i class="fa fa-check"></i><b>4.2.3</b> Spatial joining</a></li>
<li class="chapter" data-level="4.2.4" data-path="spatial-operations.html"><a href="spatial-operations.html#non-overlapping-joins"><i class="fa fa-check"></i><b>4.2.4</b> Non-overlapping joins</a></li>
<li class="chapter" data-level="4.2.5" data-path="spatial-operations.html"><a href="spatial-operations.html#spatial-aggr"><i class="fa fa-check"></i><b>4.2.5</b> Spatial data aggregation</a></li>
<li class="chapter" data-level="4.2.6" data-path="spatial-operations.html"><a href="spatial-operations.html#distance-relations"><i class="fa fa-check"></i><b>4.2.6</b> Distance relations</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="spatial-operations.html"><a href="spatial-operations.html#spatial-ras"><i class="fa fa-check"></i><b>4.3</b> Spatial operations on raster data</a><ul>
<li class="chapter" data-level="4.3.1" data-path="attr.html"><a href="attr.html#raster-subsetting"><i class="fa fa-check"></i><b>4.3.1</b> Spatial subsetting</a></li>
<li class="chapter" data-level="4.3.2" data-path="spatial-operations.html"><a href="spatial-operations.html#map-algebra"><i class="fa fa-check"></i><b>4.3.2</b> Map algebra</a></li>
<li class="chapter" data-level="4.3.3" data-path="spatial-operations.html"><a href="spatial-operations.html#local-operations"><i class="fa fa-check"></i><b>4.3.3</b> Local operations</a></li>
<li class="chapter" data-level="4.3.4" data-path="spatial-operations.html"><a href="spatial-operations.html#focal-operations"><i class="fa fa-check"></i><b>4.3.4</b> Focal operations</a></li>
<li class="chapter" data-level="4.3.5" data-path="spatial-operations.html"><a href="spatial-operations.html#zonal-operations"><i class="fa fa-check"></i><b>4.3.5</b> Zonal operations</a></li>
<li class="chapter" data-level="4.3.6" data-path="spatial-operations.html"><a href="spatial-operations.html#global-operations-and-distances"><i class="fa fa-check"></i><b>4.3.6</b> Global operations and distances</a></li>
<li class="chapter" data-level="4.3.7" data-path="spatial-operations.html"><a href="spatial-operations.html#merging-rasters"><i class="fa fa-check"></i><b>4.3.7</b> Merging rasters</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="spatial-operations.html"><a href="spatial-operations.html#exercises-2"><i class="fa fa-check"></i><b>4.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="transform.html"><a href="transform.html"><i class="fa fa-check"></i><b>5</b> Geometric operations</a><ul>
<li class="chapter" data-level="" data-path="transform.html"><a href="transform.html#prerequisites-3"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="5.1" data-path="transform.html"><a href="transform.html#introduction-2"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="transform.html"><a href="transform.html#reproj-geo-data"><i class="fa fa-check"></i><b>5.2</b> Reprojecting geographic data</a><ul>
<li class="chapter" data-level="5.2.1" data-path="transform.html"><a href="transform.html#which-crs-to-use"><i class="fa fa-check"></i><b>5.2.1</b> Which CRS to use?</a></li>
<li class="chapter" data-level="5.2.2" data-path="transform.html"><a href="transform.html#reproj-vec-geom"><i class="fa fa-check"></i><b>5.2.2</b> Reprojecting vector geometries</a></li>
<li class="chapter" data-level="5.2.3" data-path="transform.html"><a href="transform.html#modifying-map-projections"><i class="fa fa-check"></i><b>5.2.3</b> Modifying map projections</a></li>
<li class="chapter" data-level="5.2.4" data-path="transform.html"><a href="transform.html#reprojecting-raster-geometries"><i class="fa fa-check"></i><b>5.2.4</b> Reprojecting raster geometries</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="transform.html"><a href="transform.html#geo-vec"><i class="fa fa-check"></i><b>5.3</b> Geometric operations on vector data</a><ul>
<li class="chapter" data-level="5.3.1" data-path="transform.html"><a href="transform.html#simplification"><i class="fa fa-check"></i><b>5.3.1</b> Simplification</a></li>
<li class="chapter" data-level="5.3.2" data-path="transform.html"><a href="transform.html#centroids"><i class="fa fa-check"></i><b>5.3.2</b> Centroids</a></li>
<li class="chapter" data-level="5.3.3" data-path="transform.html"><a href="transform.html#buffers"><i class="fa fa-check"></i><b>5.3.3</b> Buffers</a></li>
<li class="chapter" data-level="5.3.4" data-path="transform.html"><a href="transform.html#affine-transformations"><i class="fa fa-check"></i><b>5.3.4</b> Affine transformations</a></li>
<li class="chapter" data-level="5.3.5" data-path="transform.html"><a href="transform.html#clipping"><i class="fa fa-check"></i><b>5.3.5</b> Clipping</a></li>
<li class="chapter" data-level="5.3.6" data-path="transform.html"><a href="transform.html#geometry-unions"><i class="fa fa-check"></i><b>5.3.6</b> Geometry unions</a></li>
<li class="chapter" data-level="5.3.7" data-path="transform.html"><a href="transform.html#type-trans"><i class="fa fa-check"></i><b>5.3.7</b> Type transformations</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="transform.html"><a href="transform.html#geo-ras"><i class="fa fa-check"></i><b>5.4</b> Geometric operations on raster data</a><ul>
<li class="chapter" data-level="5.4.1" data-path="transform.html"><a href="transform.html#extent-and-origin"><i class="fa fa-check"></i><b>5.4.1</b> Extent and origin</a></li>
<li class="chapter" data-level="5.4.2" data-path="transform.html"><a href="transform.html#aggregation-and-disaggregation"><i class="fa fa-check"></i><b>5.4.2</b> Aggregation and disaggregation</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="transform.html"><a href="transform.html#exercises-3"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="read-write.html"><a href="read-write.html"><i class="fa fa-check"></i><b>6</b> Geographic data I/O</a><ul>
<li class="chapter" data-level="" data-path="read-write.html"><a href="read-write.html#prerequisites-4"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="6.1" data-path="read-write.html"><a href="read-write.html#introduction-3"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="read-write.html"><a href="read-write.html#retrieving-data"><i class="fa fa-check"></i><b>6.2</b> Retrieving open data</a></li>
<li class="chapter" data-level="6.3" data-path="read-write.html"><a href="read-write.html#file-formats"><i class="fa fa-check"></i><b>6.3</b> File formats</a></li>
<li class="chapter" data-level="6.4" data-path="read-write.html"><a href="read-write.html#data-input"><i class="fa fa-check"></i><b>6.4</b> Data Input (I)</a><ul>
<li class="chapter" data-level="6.4.1" data-path="read-write.html"><a href="read-write.html#vector-data-1"><i class="fa fa-check"></i><b>6.4.1</b> Vector data</a></li>
<li class="chapter" data-level="6.4.2" data-path="read-write.html"><a href="read-write.html#raster-data-1"><i class="fa fa-check"></i><b>6.4.2</b> Raster data</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="read-write.html"><a href="read-write.html#data-output"><i class="fa fa-check"></i><b>6.5</b> Data output (O)</a><ul>
<li class="chapter" data-level="6.5.1" data-path="read-write.html"><a href="read-write.html#vector-data-2"><i class="fa fa-check"></i><b>6.5.1</b> Vector data</a></li>
<li class="chapter" data-level="6.5.2" data-path="read-write.html"><a href="read-write.html#raster-data-2"><i class="fa fa-check"></i><b>6.5.2</b> Raster data</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="read-write.html"><a href="read-write.html#visual-outputs"><i class="fa fa-check"></i><b>6.6</b> Visual outputs</a></li>
<li class="chapter" data-level="6.7" data-path="read-write.html"><a href="read-write.html#exercises-4"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>II Applied geocomputation</b></span></li>
<li class="chapter" data-level="7" data-path="transport.html"><a href="transport.html"><i class="fa fa-check"></i><b>7</b> Transport applications</a><ul>
<li class="chapter" data-level="" data-path="transport.html"><a href="transport.html#prerequisites-5"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="7.1" data-path="transport.html"><a href="transport.html#introduction-4"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="transport.html"><a href="transport.html#bris-case"><i class="fa fa-check"></i><b>7.2</b> A case study of Bristol</a></li>
<li class="chapter" data-level="7.3" data-path="transport.html"><a href="transport.html#transport-zones"><i class="fa fa-check"></i><b>7.3</b> Transport zones</a></li>
<li class="chapter" data-level="7.4" data-path="transport.html"><a href="transport.html#desire-lines"><i class="fa fa-check"></i><b>7.4</b> Desire lines</a></li>
<li class="chapter" data-level="7.5" data-path="transport.html"><a href="transport.html#routes"><i class="fa fa-check"></i><b>7.5</b> Routes</a></li>
<li class="chapter" data-level="7.6" data-path="transport.html"><a href="transport.html#nodes"><i class="fa fa-check"></i><b>7.6</b> Nodes</a></li>
<li class="chapter" data-level="7.7" data-path="transport.html"><a href="transport.html#route-networks"><i class="fa fa-check"></i><b>7.7</b> Route networks</a></li>
<li class="chapter" data-level="7.8" data-path="transport.html"><a href="transport.html#prioritizing-new-infrastructure"><i class="fa fa-check"></i><b>7.8</b> Prioritizing new infrastructure</a></li>
<li class="chapter" data-level="7.9" data-path="transport.html"><a href="transport.html#future-directions-of-travel"><i class="fa fa-check"></i><b>7.9</b> Future directions of travel</a></li>
<li class="chapter" data-level="7.10" data-path="transport.html"><a href="transport.html#ex-transport"><i class="fa fa-check"></i><b>7.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="location.html"><a href="location.html"><i class="fa fa-check"></i><b>8</b> Location analysis</a><ul>
<li class="chapter" data-level="" data-path="location.html"><a href="location.html#prerequisites-6"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="8.1" data-path="location.html"><a href="location.html#introduction-5"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="location.html"><a href="location.html#case-study"><i class="fa fa-check"></i><b>8.2</b> Case study: bike shops in Germany</a></li>
<li class="chapter" data-level="8.3" data-path="location.html"><a href="location.html#tidy-the-input-data"><i class="fa fa-check"></i><b>8.3</b> Tidy the input data</a></li>
<li class="chapter" data-level="8.4" data-path="location.html"><a href="location.html#create-census-rasters"><i class="fa fa-check"></i><b>8.4</b> Create census rasters</a></li>
<li class="chapter" data-level="8.5" data-path="location.html"><a href="location.html#define-metropolitan-areas"><i class="fa fa-check"></i><b>8.5</b> Define metropolitan areas</a></li>
<li class="chapter" data-level="8.6" data-path="location.html"><a href="location.html#points-of-interest"><i class="fa fa-check"></i><b>8.6</b> Points of interest</a></li>
<li class="chapter" data-level="8.7" data-path="location.html"><a href="location.html#identifying-suitable-locations"><i class="fa fa-check"></i><b>8.7</b> Identifying suitable locations</a></li>
<li class="chapter" data-level="8.8" data-path="location.html"><a href="location.html#discussion-and-next-steps"><i class="fa fa-check"></i><b>8.8</b> Discussion and next steps</a></li>
<li class="chapter" data-level="8.9" data-path="location.html"><a href="location.html#exercises-5"><i class="fa fa-check"></i><b>8.9</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Advanced methods</b></span></li>
<li class="chapter" data-level="9" data-path="adv-map.html"><a href="adv-map.html"><i class="fa fa-check"></i><b>9</b> Making maps with R</a><ul>
<li class="chapter" data-level="" data-path="adv-map.html"><a href="adv-map.html#prerequisites-7"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="9.1" data-path="adv-map.html"><a href="adv-map.html#introduction-6"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="adv-map.html"><a href="adv-map.html#static-maps"><i class="fa fa-check"></i><b>9.2</b> Static maps</a><ul>
<li class="chapter" data-level="9.2.1" data-path="adv-map.html"><a href="adv-map.html#tmap-basics"><i class="fa fa-check"></i><b>9.2.1</b> tmap basics</a></li>
<li class="chapter" data-level="9.2.2" data-path="adv-map.html"><a href="adv-map.html#map-objects-shapes-and-layers"><i class="fa fa-check"></i><b>9.2.2</b> Map objects, shapes and layers</a></li>
<li class="chapter" data-level="9.2.3" data-path="adv-map.html"><a href="adv-map.html#aesthetics"><i class="fa fa-check"></i><b>9.2.3</b> Aesthetics</a></li>
<li class="chapter" data-level="9.2.4" data-path="adv-map.html"><a href="adv-map.html#map-layouts"><i class="fa fa-check"></i><b>9.2.4</b> Map layouts</a></li>
<li class="chapter" data-level="9.2.5" data-path="adv-map.html"><a href="adv-map.html#class-intervals"><i class="fa fa-check"></i><b>9.2.5</b> Class intervals</a></li>
<li class="chapter" data-level="9.2.6" data-path="adv-map.html"><a href="adv-map.html#faceted-maps"><i class="fa fa-check"></i><b>9.2.6</b> Faceted maps</a></li>
<li class="chapter" data-level="9.2.7" data-path="adv-map.html"><a href="adv-map.html#inset-maps"><i class="fa fa-check"></i><b>9.2.7</b> Inset maps</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="adv-map.html"><a href="adv-map.html#other-static-mapping-packages"><i class="fa fa-check"></i><b>9.3</b> Other static mapping packages</a></li>
<li class="chapter" data-level="9.4" data-path="adv-map.html"><a href="adv-map.html#animated-maps"><i class="fa fa-check"></i><b>9.4</b> Animated maps</a></li>
<li class="chapter" data-level="9.5" data-path="adv-map.html"><a href="adv-map.html#interactive-maps"><i class="fa fa-check"></i><b>9.5</b> Interactive maps</a></li>
<li class="chapter" data-level="9.6" data-path="adv-map.html"><a href="adv-map.html#pseudo-unusual-maps"><i class="fa fa-check"></i><b>9.6</b> Pseudo (unusual) maps</a></li>
<li class="chapter" data-level="9.7" data-path="adv-map.html"><a href="adv-map.html#web-mapping-applications-with-shiny"><i class="fa fa-check"></i><b>9.7</b> Web mapping applications with shiny</a></li>
<li class="chapter" data-level="9.8" data-path="adv-map.html"><a href="adv-map.html#exercises-6"><i class="fa fa-check"></i><b>9.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="gis.html"><a href="gis.html"><i class="fa fa-check"></i><b>10</b> Bridges to GIS software</a><ul>
<li class="chapter" data-level="10.1" data-path="gis.html"><a href="gis.html#rqgis"><i class="fa fa-check"></i><b>10.1</b> (R)QGIS</a></li>
<li class="chapter" data-level="10.2" data-path="gis.html"><a href="gis.html#rsaga"><i class="fa fa-check"></i><b>10.2</b> (R)SAGA</a></li>
<li class="chapter" data-level="10.3" data-path="gis.html"><a href="gis.html#grass-through-rgrass7"><i class="fa fa-check"></i><b>10.3</b> GRASS through <strong>rgrass7</strong></a></li>
<li class="chapter" data-level="10.4" data-path="gis.html"><a href="gis.html#when-to-use-what"><i class="fa fa-check"></i><b>10.4</b> When to use what?</a></li>
<li class="chapter" data-level="10.5" data-path="gis.html"><a href="gis.html#exercises-7"><i class="fa fa-check"></i><b>10.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="raster-vector.html"><a href="raster-vector.html"><i class="fa fa-check"></i><b>11</b> Raster-vector interactions</a><ul>
<li class="chapter" data-level="" data-path="raster-vector.html"><a href="raster-vector.html#prerequisites-8"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="11.1" data-path="raster-vector.html"><a href="raster-vector.html#introduction-7"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="raster-vector.html"><a href="raster-vector.html#raster-cropping"><i class="fa fa-check"></i><b>11.2</b> Raster cropping</a></li>
<li class="chapter" data-level="11.3" data-path="raster-vector.html"><a href="raster-vector.html#raster-extraction"><i class="fa fa-check"></i><b>11.3</b> Raster extraction</a></li>
<li class="chapter" data-level="11.4" data-path="raster-vector.html"><a href="raster-vector.html#rasterization"><i class="fa fa-check"></i><b>11.4</b> Rasterization</a></li>
<li class="chapter" data-level="11.5" data-path="raster-vector.html"><a href="raster-vector.html#spatial-vectorization"><i class="fa fa-check"></i><b>11.5</b> Spatial vectorization</a></li>
<li class="chapter" data-level="11.6" data-path="raster-vector.html"><a href="raster-vector.html#exercises-8"><i class="fa fa-check"></i><b>11.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="algorithms-and-functions-for-geocomputation.html"><a href="algorithms-and-functions-for-geocomputation.html"><i class="fa fa-check"></i><b>12</b> Algorithms and functions for geocomputation</a><ul>
<li class="chapter" data-level="" data-path="algorithms-and-functions-for-geocomputation.html"><a href="algorithms-and-functions-for-geocomputation.html#prerequisites-9"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="12.1" data-path="algorithms-and-functions-for-geocomputation.html"><a href="algorithms-and-functions-for-geocomputation.html#geographic-algorithms"><i class="fa fa-check"></i><b>12.1</b> Geographic algorithms</a></li>
<li class="chapter" data-level="12.2" data-path="algorithms-and-functions-for-geocomputation.html"><a href="algorithms-and-functions-for-geocomputation.html#functions"><i class="fa fa-check"></i><b>12.2</b> Functions</a></li>
<li class="chapter" data-level="12.3" data-path="algorithms-and-functions-for-geocomputation.html"><a href="algorithms-and-functions-for-geocomputation.html#implementation"><i class="fa fa-check"></i><b>12.3</b> Implementation</a></li>
<li class="chapter" data-level="12.4" data-path="location.html"><a href="location.html#case-study"><i class="fa fa-check"></i><b>12.4</b> Case study</a></li>
<li class="chapter" data-level="12.5" data-path="algorithms-and-functions-for-geocomputation.html"><a href="algorithms-and-functions-for-geocomputation.html#exercises-9"><i class="fa fa-check"></i><b>12.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="spatial-cv.html"><a href="spatial-cv.html"><i class="fa fa-check"></i><b>13</b> Spatial cross-validation for machine learning</a><ul>
<li class="chapter" data-level="" data-path="spatial-cv.html"><a href="spatial-cv.html#prerequisites-10"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="13.1" data-path="spatial-cv.html"><a href="spatial-cv.html#introduction-8"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="spatial-cv.html"><a href="spatial-cv.html#case-landslide"><i class="fa fa-check"></i><b>13.2</b> Case study: landslide susceptibility</a></li>
<li class="chapter" data-level="13.3" data-path="spatial-cv.html"><a href="spatial-cv.html#conventional-model"><i class="fa fa-check"></i><b>13.3</b> Conventional modeling approach in R</a></li>
<li class="chapter" data-level="13.4" data-path="spatial-cv.html"><a href="spatial-cv.html#intro-cv"><i class="fa fa-check"></i><b>13.4</b> Introduction to (spatial) cross-validation</a></li>
<li class="chapter" data-level="13.5" data-path="spatial-cv.html"><a href="spatial-cv.html#spatial-cv-with-mlr"><i class="fa fa-check"></i><b>13.5</b> Spatial CV with <strong>mlr</strong></a><ul>
<li class="chapter" data-level="13.5.1" data-path="spatial-cv.html"><a href="spatial-cv.html#glm"><i class="fa fa-check"></i><b>13.5.1</b> Generalized linear model</a></li>
<li class="chapter" data-level="13.5.2" data-path="spatial-cv.html"><a href="spatial-cv.html#svm"><i class="fa fa-check"></i><b>13.5.2</b> (Spatial) Tuning of machine-learning hyperparameters</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="spatial-cv.html"><a href="spatial-cv.html#conclusions"><i class="fa fa-check"></i><b>13.6</b> Conclusions</a></li>
<li class="chapter" data-level="13.7" data-path="spatial-cv.html"><a href="spatial-cv.html#exercises-10"><i class="fa fa-check"></i><b>13.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="http://robinlovelace.net/">Robin Lovelace</a></li>
<li><a href="https://nowosad.github.io/">Jakub Nowosad</a></li>
<li><a href="http://www.geographie.uni-jena.de/en/Muenchow.html">Jannes Muenchow</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Geocomputation with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="spatial-cv" class="section level1">
<h1><span class="header-section-number">13</span> Spatial cross-validation for machine learning</h1>
<div id="prerequisites-10" class="section level2 unnumbered">
<h2>Prerequisites</h2>
<p>This chapter requires a strong grasp of spatial data analysis and processing, covered in chapters <a href="spatial-class.html#spatial-class">2</a> to <a href="transform.html#transform">5</a>. You should also be familiar with linear regression, its generalized extensions and machine learning <span class="citation">(e.g. Zuur et al. <a href="#ref-zuur_mixed_2009">2009</a>; James et al. <a href="#ref-james_introduction_2013">2013</a>)</span>.</p>
<p>The chapter uses the following packages:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(mlr)
<span class="kw">library</span>(pROC)
<span class="kw">library</span>(raster)
<span class="kw">library</span>(RSAGA)
<span class="kw">library</span>(sf)
<span class="kw">library</span>(tidyverse)</code></pre></div>
<ul>
<li>Required data will be downloaded in due course.</li>
</ul>
</div>
<div id="introduction-8" class="section level2">
<h2><span class="header-section-number">13.1</span> Introduction</h2>
<p>Section <a href="intro.html#software-for-geocomputation">1.3</a> mentioned several programming languages suitable for command-line based geocomputation. The advantages of geocomputation with R were discussed, including its unparalleled statistical power. This chapter makes use of some of this statistical power, by demonstrating methods for predictive mapping by means of statistical learning <span class="citation">(James et al. <a href="#ref-james_introduction_2013">2013</a>)</span>. The main focus, however, is the use of spatial cross-validation (or ‘spatial CV’ for short, a term we will define shortly) to assess model performance and reduce spatial bias. Spatial CV is an excellent example of using statistical methods to model spatial data and, at the time of writing, the technique is better supported in R than any other language.</p>
<p>Statistical learning comprises a large suite of techniques for understanding data. Statistical learning can be roughly grouped into supervised and unsupervised techniques, both of which are used throughout a vast range of disciplines including economics, physics, medicine, biology, ecology and geography <span class="citation">(James et al. <a href="#ref-james_introduction_2013">2013</a>)</span>. In this chapter we will focus on supervised techniques, i.e., we have a response variable, in our case this will be a binary one (landslide vs. non-landslide occurrence) but could be also a numeric (pH value), an integer (species richness) or a categorical variable (land use). Supervised techniques model the relationship between the response variable and various predictors. For this we can use techniques from the field of statistics or from the field of machine learning. Which to use depends on the aim: statistical inference or prediction. (Semi-)Parametric regression techniques are especially useful if the aim is statistical inference, i.e. if we are interested in a predictor’s significance, its importance for a specific model and to explain relationships between response and predictors. To trust the p-values and standard errors of such models we need to perform a thorough model validation testing if one or several of the underlying model assumptions (heterogeneity, independence, etc.) have been violated <span class="citation">(Zuur et al. <a href="#ref-zuur_mixed_2009">2009</a>)</span>. By contrast, machine learning aims at predictions and is especially appealing due its lack of assumptions. Though statistical inference is impossible <span class="citation">(James et al. <a href="#ref-james_introduction_2013">2013</a>)</span>, various studies have shown that machine learning is at least at par with regression techniques regarding predictive performance <span class="citation">(e.g., Schratz et al. <a href="#ref-schratz_performance_nodate">2018</a>)</span>. <!-- add one more source --> Naturally, with the advent of big data, machine learning has even gained in popularity since frequently the underlying relationship between variables is less important than the prediction. Think for instance of future customer behavior, recommendation services (music, movies, what to by next), face recognition, autonomous driving and classifying e-mails as spam to name but a few. Note that we can borrow regression techniques from statistics for machine learning when the aim is prediction. In this case we do not have too worry too much about possible model misspecifications since we explicitly do not want to do statistical inference. In fact, in this chapter the aim will be the (spatial) prediction of landslide susceptibility. We will start out with a generalized linear model (GLM) which is probably familiar to most readers<a href="#fn60" class="footnoteRef" id="fnref60"><sup>60</sup></a>, before moving on to using a typical machine learning algorithm, namely a support vector machine. Instead of explaining in detail the models we will focus on the specialty of geographic data in a modeling context and the models’ predictive performance via spatial CV.</p>
<p>CV determines a model’s ability to predict new data or differently put its ability to generalize. To achieve this, CV splits a dataset (repeatedly) into test and training sets. It uses the training data to fit the model, and checks if the trained model is able to predict the correct results for the test data. Basically, cross-validation helps to detect over-fitting since a model that fits too closely the training data and its specific peculiarities (noise, random fluctuations) will have a bad prediction performance on the test data. However, the basic requirement for this is, that the test data is independent of the training data. CV achieves this by splitting the data randomly into test and training sets. However, randomly splitting spatial data results in the fact that training points are frequently located next to test points. Since points close to each other are more similar compared to points further away, test and training datasets might not be independent. The consequence is that cross-validation would fail to detect over-fitting in the presence of spatial autocorrelation. Here, spatial CV will come to the rescue which will be the main topic of this chapter.</p>
</div>
<div id="case-landslide" class="section level2">
<h2><span class="header-section-number">13.2</span> Case study: landslide susceptibility</h2>
<p>To introduce spatial CV by example, we will use a landslide dataset from Southern Ecuador (Figure <a href="spatial-cv.html#fig:lsl-map">13.1</a>). For a detailed description of the dataset and the study area please refer to <span class="citation">Muenchow, Brenning, and Richter (<a href="#ref-muenchow_geomorphic_2012">2012</a>)</span>. One can find a subset of the corresponding data in the <strong>RSAGA</strong> package. The following command loads three datasets, a <code>data.frame</code> named <code>landslides</code>, a <code>list</code> named <code>dem</code>, and an <code>sf</code>-object named <code>study_area</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">&quot;landslides&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;RSAGA&quot;</span>)</code></pre></div>
<p><code>landslides</code> contains a factor column <code>lslpts</code> where <code>TRUE</code> corresponds to an observed landslide initiation point and <code>FALSE</code> to points where no landsliding occurred. Columns <code>x</code> and <code>y</code> contain the corresponding coordinates. The landslide initiation point is located in the scarp of a landslide polygon. The coordinates for the non-landslide points were sampled randomly with the restriction to fall outside of the slightly buffered landslide polygons. <code>summary(landslides$lslpts)</code> tells us that 175 landslide points and 1360 non-landslide points are available. To make the ratio between landslide and non-landslide points more balanced, we randomly sample 175 from the 1360 non-landslide points.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># select non-landslide points</span>
non =<span class="st"> </span><span class="kw">filter</span>(landslides, lslpts ==<span class="st"> </span><span class="ot">FALSE</span>)
<span class="co"># select landslide points</span>
lsl_pts =<span class="st"> </span><span class="kw">filter</span>(landslides, lslpts ==<span class="st"> </span><span class="ot">TRUE</span>)
<span class="co"># randomly select 175 non-landslide points</span>
ind =<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span>:<span class="kw">nrow</span>(non), <span class="kw">nrow</span>(lsl_pts))
<span class="co"># rowbind randomly selected non-landslide points and </span>
<span class="co"># landslide points</span>
lsl =<span class="st"> </span><span class="kw">rbind</span>(non[ind, ], lsl_pts)</code></pre></div>
<p><code>dem</code> is in fact a digital elevation model and consists of two list elements with the first being a raster header and the second being a matrix containing the altitudinal values. To transform this list into a <code>raster</code> object, we can write:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dem =<span class="st"> </span>
<span class="st">  </span><span class="kw">raster</span>(dem$data, 
         <span class="dt">crs =</span> dem$header$proj4string,
         <span class="dt">xmn =</span> dem$header$xllcorner, 
         <span class="dt">xmx =</span> dem$header$xllcorner +<span class="st"> </span>
<span class="st">           </span>dem$header$ncols *<span class="st"> </span>dem$header$cellsize,
         <span class="dt">ymn =</span> dem$header$yllcorner,
         <span class="dt">ymx =</span> dem$header$yllcorner +<span class="st"> </span>
<span class="st">           </span>dem$header$nrows *<span class="st"> </span>dem$header$cellsize)</code></pre></div>
<p>To model the probability for landslide occurrence, we need some predictors. We will use selected terrain attributes frequently associated with landsliding <span class="citation">(Muenchow, Brenning, and Richter <a href="#ref-muenchow_geomorphic_2012">2012</a>)</span>, all of which can be computed from the provided digital elevation model (<code>dem</code>) using R-GIS bridges (see Chapter <a href="gis.html#gis">10</a>). We leave it as an exercise to the reader to compute the terrain attribute rasters and extract the corresponding values to our landslide/non-landslide dataframe (see also exercises). The first three rows of the resulting dataframe (still named <code>lsl</code>) excluding the coordinates could look like this: <!-- has anybody an idea why I have to run the following code chunk two times to make it work when rendering the book with `bookdown::render_book()`?--></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dplyr::<span class="kw">select</span>(lsl, -x, -y) %&gt;%
<span class="st">  </span><span class="kw">head</span>(., <span class="dv">3</span>)
<span class="co">#&gt;      lslpts slope    cplan    cprof elev log_carea</span>
<span class="co">#&gt; 1337  FALSE  39.7  0.02512 -0.00311 2357      2.66</span>
<span class="co">#&gt; 384   FALSE  55.7 -0.03212 -0.00491 2093      2.96</span>
<span class="co">#&gt; 1551  FALSE  36.7 -0.00347  0.00615 1815      2.82</span></code></pre></div>
<p>The added columns are:</p>
<ul>
<li><code>slope</code>: slope angle (°)</li>
<li><code>cplan</code>: plan curvature (rad m<sup>−1</sup>) expressing the convergence or divergence of a slope and thus water flow.</li>
<li><code>cprof</code>: profile curvature (rad m<sup>-1</sup>) as a measure of flow acceleration, also known as downslope change in slope angle</li>
<li><code>elev</code>: elevation (m a.s.l.) as the representation of different altitudinal zones of vegetation and precipitation in the study area.</li>
<li><code>log_carea</code>: the decadic logarithm of the catchment area (log m<sup>2</sup>) representing the amount of water flowing towards a location.</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:lsl-map"></span>
<img src="figures/lsl-map-1.png" alt="Landslide initiation points (red) and points unaffected by landsliding (blue) in Southern Ecuador. CRS: UTM zone 17S (EPSG: 32717)." width="576" />
<p class="caption">
Figure 13.1: Landslide initiation points (red) and points unaffected by landsliding (blue) in Southern Ecuador. CRS: UTM zone 17S (EPSG: 32717).
</p>
</div>
</div>
<div id="conventional-model" class="section level2">
<h2><span class="header-section-number">13.3</span> Conventional modeling approach in R</h2>
<p>Later on we will introduce the <strong>mlr</strong> package, an umbrella-package providing a unified interface to hundreds of modeling approaches. Before doing so, it is worth taking a look at the conventional modeling interface in R. This way we introduce statistical supervised modeling in R which provides the required skill set for doing spatial CV and additionally contributes to a better grasp on the <strong>mlr</strong> approach introduced later on. Usually, we model the response variable as a function of predictors. Therefore, modeling functions in R such as <code>lm</code>, <code>glm</code> and many more use the so-called formula interface. Let’s put this into practice by modeling the landslide occurrence as a function of terrain attributes. Since our response (landslide occurrence) belongs to the binary category, we use a binomial generalized linear model instead of a simple linear model which would expect a normally distributed numeric response variable.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit =<span class="st"> </span><span class="kw">glm</span>(lslpts ~<span class="st"> </span>slope +<span class="st"> </span>cplan +<span class="st"> </span>cprof +<span class="st"> </span>elev +<span class="st"> </span>log_carea, 
          <span class="dt">data =</span> lsl, <span class="dt">family =</span> <span class="kw">binomial</span>())
<span class="co"># the same as:</span>
<span class="co"># fit = glm(lslpts ~ ., data = select(lsl, -x, -y))</span>
fit
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:  glm(formula = lslpts ~ slope + cplan + cprof + elev + log_carea, </span>
<span class="co">#&gt;     family = binomial(), data = lsl)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Coefficients:</span>
<span class="co">#&gt; (Intercept)        slope        cplan        cprof         elev  </span>
<span class="co">#&gt;    2.39e+00     1.22e-01    -1.96e+01    -2.22e+01    -3.98e-04  </span>
<span class="co">#&gt;   log_carea  </span>
<span class="co">#&gt;   -2.32e+00  </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Degrees of Freedom: 349 Total (i.e. Null);  344 Residual</span>
<span class="co">#&gt; Null Deviance:       485 </span>
<span class="co">#&gt; Residual Deviance: 356   AIC: 368</span></code></pre></div>
<p>Subsequently, we can use the estimated model coefficients for predictions. The generic <code>predict()</code> function does this automatically for us. The <code>response</code> option gives back the predicted probabilities (of landslide occurrence) for each observation in <code>lsl</code> (see <code>?predict.glm</code>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">predict</span>(<span class="dt">object =</span> fit, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>))
<span class="co">#&gt;  1337   384  1551  1555   209  1135 </span>
<span class="co">#&gt; 0.431 0.904 0.387 0.272 0.159 0.784</span></code></pre></div>
<p>We can also predict spatially by applying the coefficients to our predictor rasters. We could do this manually but can also use <strong>raster</strong>’s <code>predict()</code> function. This function also expects the fitted model as input as well as a raster stack with the predictors exactly named as in the fitted model (Figure <a href="spatial-cv.html#fig:lsl-susc">13.2</a>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred =<span class="st"> </span>raster::<span class="kw">predict</span>(<span class="dt">object =</span> ta, <span class="dt">model =</span> fit,
                       <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:lsl-susc"></span>
<img src="figures/lsl-susc-1.png" alt="Spatial prediction of landslide susceptibility using a  GLM. CRS: UTM zone 17S (EPSG: 32717)." width="576" />
<p class="caption">
Figure 13.2: Spatial prediction of landslide susceptibility using a GLM. CRS: UTM zone 17S (EPSG: 32717).
</p>
</div>
<p>Here, when making predictions we neglect spatial autocorrelation since we assume that on average predictive accuracies of parametric models remain the same with or without spatial autocorrelation structures. However, it is possible to include spatial autocorrelation structures into models as well as into the predictions. This is, however, beyond the scope of this book. Nevertheless, we give the interested reader some pointers where to look it up. There are three main directions:</p>
<ol style="list-style-type: decimal">
<li>The predictions of universal kriging are the predictions of a simple linear model plus the kriged model’s residuals, i.e. spatially interpolated residuals <span class="citation">(Bivand, Pebesma, and Gómez-Rubio <a href="#ref-bivand_applied_2013">2013</a>)</span>.</li>
<li>Adding a spatial correlation (dependency) structure to a generalized least squares model <span class="citation">(<code>nlme::gls()</code>; Zuur et al. <a href="#ref-zuur_mixed_2009">2009</a>; Zuur et al. <a href="#ref-zuur_beginners_2017">2017</a>)</span>.<a href="#fn61" class="footnoteRef" id="fnref61"><sup>61</sup></a></li>
<li>Finally, there are mixed-effect modeling approaches. Basically, a random effect imposes a dependency structure on the response variable which in turn allows for observations of one class to be more similar to each other than to those of another class <span class="citation">(Zuur et al. <a href="#ref-zuur_mixed_2009">2009</a>)</span>. Classes can be for example bee hives, owl nests, vegetation transects or an altitudinal stratification. This mixed modeling approach assumes normal and independent distributed random intercepts.<a href="#fn62" class="footnoteRef" id="fnref62"><sup>62</sup></a> This can even be extended by using a random intercept that is normal and spatially dependent. For this, however, you will have to resort most likely to Bayesian modeling approaches since frequentist software tools are rather limited in this respect especially for more complex models <span class="citation">(Blangiardo and Cameletti <a href="#ref-blangiardo_spatial_2015">2015</a>; Zuur et al. <a href="#ref-zuur_beginners_2017">2017</a>)</span>.</li>
</ol>
<p>Spatial predictions are one very important outcome of a model. Even more important is how good the model is at making them since the most beautiful prediction map is useless if a model’s predictive performance is bad. The most popular measure to assess the predictive performance of a binomial model is the Area Under the Receiver Operator Characteristic Curve (AUROC). This is a value between 0.5 and 1.0 with 0.5 indicating no and 1.0 indicating a perfect discrimination of the two classes. Thus, the higher the AUROC the better is our model at making predictions. In the following we compute the receiver operator characteristic with the help of <code>roc()</code> by providing it with the response variable and the predicted values. <code>auc()</code> returns the area under the curve.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pROC::<span class="kw">auc</span>(pROC::<span class="kw">roc</span>(lsl$lslpts, <span class="kw">fitted</span>(fit)))
<span class="co">#&gt; Area under the curve: 0.844</span></code></pre></div>
<p>An AUROC of 0.84 represents a good fit. However, this is an overoptimistic estimation since we have computed it on the complete dataset. To derive an biased-reduced assessment we have to use cross-validation and in the case of spatial data we will have to make use of spatial CV.</p>
</div>
<div id="intro-cv" class="section level2">
<h2><span class="header-section-number">13.4</span> Introduction to (spatial) cross-validation</h2>
<p>Cross-validation belongs to the family of resampling methods <span class="citation">(James et al. <a href="#ref-james_introduction_2013">2013</a>)</span>. The basic idea is to split (repeatedly) a dataset into training and test sets whereby the training data is used to fit a model which then is applied to the test set. Comparing the predicted values with the known response values from the test set (using a performance measure such as the AUROC in the binomial case) gives a bias-reduced assessment of the model’s capability to generalize the learned relationship to independent data. For example, a 100-repeated 5-fold cross-validation means to randomly split the data into five partitions (folds) with each fold being used once as a test set (see upper row of Figure <a href="spatial-cv.html#fig:partitioning">13.3</a>). This guarantees that each observation is used once as the test set, and requires the fitting of five models. Subsequently, this procedure is repeated 100 times. Of course, the data splitting will differ (though often only slightly) in each repetition. Overall, this amounts to fitting 500 models whereas the mean performance measure (AUROC) of all models is the model’s overall prediction power.</p>
<p>However, geographic data is special. Remember that the first law of geography states that points close to each other tend to be, on average, more similar compared to points further away (<span class="citation">Miller (<a href="#ref-miller_toblers_2004">2004</a>)</span>; Chapter <a href="transport.html#transport">7</a>). This means these points are not statistically independent or put differently that training and test points in conventional cross-validation are often too close to each other (see first row of <a href="spatial-cv.html#fig:partitioning">13.3</a>). Using this information in our modeling is like a sneak preview, i.e. using information that should be unavailable to the training dataset. To overcome this problem, we should make use of spatial partitioning which splits the observations into spatially disjoint folds (using the observations’ coordinates in a <em>k</em>-means clustering; <span class="citation">A. Brenning (<a href="#ref-brenning_spatial_2012">2012</a><a href="#ref-brenning_spatial_2012">b</a>)</span>; second row of Figure <a href="spatial-cv.html#fig:partitioning">13.3</a>). The partitioning strategy is <strong>the</strong> distinguishing feature between spatial and conventional cross-validation. Everything else remains exactly the same. As a result spatial CV leads to a bias-reduced assessment of a model’s predictive performance, and hence helps to avoid over-fitting. It is important to note that spatial CV reduces the bias introduced by spatial autocorrelation but does not completely remove it. This is because there are still a few points in the test and training data which are still neighbors (<span class="citation">A. Brenning (<a href="#ref-brenning_spatial_2012">2012</a><a href="#ref-brenning_spatial_2012">b</a>)</span>; see second row of <a href="spatial-cv.html#fig:partitioning">13.3</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:partitioning"></span>
<img src="figures/13_partitioning.png" alt="Spatial visualization of selected test and training observations for cross-validation in one repetition. Random (upper row) and spatial partitioning (lower row)." width="708" />
<p class="caption">
Figure 13.3: Spatial visualization of selected test and training observations for cross-validation in one repetition. Random (upper row) and spatial partitioning (lower row).
</p>
</div>
</div>
<div id="spatial-cv-with-mlr" class="section level2">
<h2><span class="header-section-number">13.5</span> Spatial CV with <strong>mlr</strong></h2>
<p>In R there are literally hundreds of packages available for statistical learning (e.g., have a look at the <a href="https://CRAN.R-project.org/view=MachineLearning">CRAN task machine learning</a>). In section <a href="spatial-cv.html#conventional-model">13.3</a> we used the <strong>stats</strong> package to fit a logistic regression using the <code>glm()</code> command. <code>glm()</code> uses the common R modeling interface:</p>
<ol style="list-style-type: decimal">
<li>specify the response and predictor variables via a formula object</li>
<li>build a model</li>
<li>make a prediction.</li>
</ol>
<p>However, many packages come with their own or a modified statistical learning interface which is why users frequently have to spend a lot of time to figure out the specifics of each of these packages, how to carry out cross-valdiation and hyperparameter tuning or how to compare modeling results from different packages. The <strong>mlr</strong> package acts as a meta- or umbrella-package providing a unified interface to the most popular statistical learning techniques available in R including classification, regression, survival analysis and clustering <span class="citation">(Bischl et al. <a href="#ref-bischl_mlr:_2016">2016</a>)</span>.<a href="#fn63" class="footnoteRef" id="fnref63"><sup>63</sup></a> The standardized <strong>mlr</strong> interface is based on so-called basic building blocks (Figure <a href="spatial-cv.html#fig:building-blocks">13.4</a>).</p>
<!-- @Jakub: yes, I will ask if we me may use the figure -->
<div class="figure" style="text-align: center"><span id="fig:building-blocks"></span>
<img src="figures/13_ml_abstraction_crop.png" alt="Basic building blocks of the **mlr** package. Source: [openml.github.io](http://openml.github.io/articles/slides/useR2017_tutorial/slides_tutorial_files/ml_abstraction-crop.png)." width="862" />
<p class="caption">
Figure 13.4: Basic building blocks of the <strong>mlr</strong> package. Source: <a href="http://openml.github.io/articles/slides/useR2017_tutorial/slides_tutorial_files/ml_abstraction-crop.png">openml.github.io</a>.
</p>
</div>
<p>First, we need to create a <strong>task</strong> containing the data, specifically the response and predictor variables, for the model and the model type (such as regression or classification). Secondly, a <strong>learner</strong> defines the specific model that models the task data or differently put learns a structure inherent in the provided data. Thirdly, we assess the predictive performance of the model, i.e. the model’s ability to generalize the learned relationship to new data via a repetitive <strong>resampling</strong> approach (see also section <a href="spatial-cv.html#intro-cv">13.4</a>).</p>
<div id="glm" class="section level3">
<h3><span class="header-section-number">13.5.1</span> Generalized linear model</h3>
<p>To put the <strong>mlr</strong> approach into practice, we first create a <strong>task</strong> using our landslide data. Since we have a binary response, which is in fact a two-category variable, we will make use of the classification task, namely <code>makeClassifTask()</code>.<a href="#fn64" class="footnoteRef" id="fnref64"><sup>64</sup></a> First, we specify the data which will be used. The <code>target</code> parameter expects the response variable and the <code>positive</code> parameter determines which of the two factor levels of the response variable indicate the landslide initiation point. All other variables of the provided dataset will serve as predictors (check out with <code>getTaskFormula(task)</code>). As we will perform a spatial CV later on, we need to specify the coordinates which will form the basis of the spatial partitioning (see section <a href="spatial-cv.html#intro-cv">13.4</a> and Figure <a href="spatial-cv.html#fig:partitioning">13.3</a>). These have to be provided in a separate dataframe object in parameter <code>coordinates</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(mlr)
<span class="co"># separate data to be modeled and coordinates</span>
coords =<span class="st"> </span>lsl[, <span class="kw">c</span>(<span class="st">&quot;x&quot;</span>, <span class="st">&quot;y&quot;</span>)]
data =<span class="st"> </span>dplyr::<span class="kw">select</span>(lsl, -x, -y)
<span class="co"># create task</span>
task =<span class="st"> </span><span class="kw">makeClassifTask</span>(<span class="dt">data =</span> data, <span class="dt">target =</span> <span class="st">&quot;lslpts&quot;</span>,
                       <span class="dt">positive =</span> <span class="st">&quot;TRUE&quot;</span>, <span class="dt">coordinates =</span> coords)</code></pre></div>
<p><code>makeLearner()</code> determines the statistical learning method to use. All classification <strong>learners</strong> start with <code>classif.</code> and all regression learners with <code>regr.</code> (see <code>?makeLearners</code> for more details). <code>listLearners()</code> helps to find out about all available learners and from which package <strong>mlr</strong> imports them. For a specific task, we can run:</p>
<!-- no idea, why render_book() fails frequently because function listLearners() cannot be found...:
I also have this issue (RL - so not running and hardcoding result)-->
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lrns =<span class="st"> </span>mlr::<span class="kw">listLearners</span>(task)
dplyr::<span class="kw">select</span>(lrns, class, name, package) %&gt;%
<span class="st">  </span>head
<span class="co">#&gt;                 class                         name package</span>
<span class="co">#&gt; 1    classif.binomial          Binomial Regression   stats</span>
<span class="co">#&gt; 2 classif.featureless       Featureless classifier     mlr</span>
<span class="co">#&gt; 3         classif.fnn     Fast k-Nearest Neighbour     FNN</span>
<span class="co">#&gt; 4         classif.knn           k-Nearest Neighbor   class</span>
<span class="co">#&gt; 5         classif.lda Linear Discriminant Analysis    MASS</span>
<span class="co">#&gt; 6      classif.logreg          Logistic Regression   stats</span></code></pre></div>
<p>This yields all learners able to model two-class problems (landslide yes or no). We opt for the binomial classification method from the <strong>stats</strong> package which we already have used in section <a href="spatial-cv.html#conventional-model">13.3</a> and is implemented as <code>classif.binomial</code> in <strong>mlr</strong>. Additionally, we have to specify the link-function. We choose the <code>logit</code> link which is also the default when using the <code>binomial</code> family in <code>glm</code> (run <code>binomial()</code> to verify). <code>predict.type</code> determines the type of the prediction with <!-- Setting it to `response` produces class labels as output, which would be in our case `TRUE` or `FALSE`. --> <code>prob</code> resulting in a predicted probability for landslide occurrence between 0 and 1.<a href="#fn65" class="footnoteRef" id="fnref65"><sup>65</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lrn =<span class="st"> </span><span class="kw">makeLearner</span>(<span class="dt">cl =</span> <span class="st">&quot;classif.binomial&quot;</span>,
                  <span class="dt">link =</span> <span class="st">&quot;logit&quot;</span>,
                  <span class="dt">predict.type =</span> <span class="st">&quot;prob&quot;</span>,
                  <span class="dt">fix.factors.prediction =</span> <span class="ot">TRUE</span>)
<span class="co"># run the following lines to find out from which package the</span>
<span class="co"># learner is taken and how to access the corresponding help </span>
<span class="co"># file(s)</span>
<span class="co"># getLearnerPackages(learner)</span>
<span class="co"># helpLearner(learner)</span></code></pre></div>
<!--
Having specified a learner and a task, we can train our model which basically executes the `glm()` command in the background for our task. 


```r
mod = train(learner = lrn, task = task)
mlr_fit = getLearnerModel(mod)
```



`getLearnerModel()` extracts the used model which shows that **mlr** passed all specified parameters to the `glm` function in the background as also proved by following code:


```r
fit = glm(lslpts ~ ., family = binomial(link = "logit"), data = data)
identical(fit$coefficients, mlr_fit$coefficients)
#> [1] TRUE
```
-->
<p>In the beginning, it might seem a bit tedious to learn the <strong>mlr</strong> interface for modeling. But remember that one only has to learn one single interface to run many learners (<strong>mlr</strong> package version: 2.13). Further advantages are the easy parallelization of resampling techniques and the tuning of machine learning hyperparameters, also spatially, in an inner fold (see section <a href="spatial-cv.html#svm">13.5.2</a>). Most importantly, (spatial) resampling in <strong>mlr</strong> is really easy, and only requires two more steps The first thing to do is specifying a resampling method. We will use a 100-repeated 5-fold spatial CV. This ensures that a spatial partitioning with five partitions is chosen based on the provided coordinates in our <code>task</code> and that the partitioning is repeated 100 times. Please note that package <strong>sperrorest</strong> initially implemented spatial cross-validation in R <span class="citation">(A. Brenning <a href="#ref-brenning_spatial_2012">2012</a><a href="#ref-brenning_spatial_2012">b</a>)</span>. In the meantime, its functionality was integrated into the <strong>mlr</strong> package which is the reason why we are using <strong>mlr</strong>.<a href="#fn66" class="footnoteRef" id="fnref66"><sup>66</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">resampling =<span class="st"> </span><span class="kw">makeResampleDesc</span>(<span class="dt">method =</span> <span class="st">&quot;SpRepCV&quot;</span>, <span class="dt">folds =</span> <span class="dv">5</span>, 
                              <span class="dt">reps =</span> <span class="dv">100</span>)</code></pre></div>
<p>To execute the spatial resampling, we run <code>resample()</code> using the specified learner, task, resampling strategy and of course the performance measure, here the AUROC. This takes a short while because we ask R to compute the AUROC from 500 models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">012348</span>)
sp_cv =<span class="st"> </span>mlr::<span class="kw">resample</span>(<span class="dt">learner =</span> lrn, <span class="dt">task =</span> task,
                      <span class="dt">resampling =</span> resampling, 
                      <span class="dt">measures =</span> mlr::auc)</code></pre></div>
<!-- sp_cv and conv_cv have been saved in spatialcv.Rdata. I needed to run the modeling outside of the book since knitr sets its own seed and I am not sure if this actually helps to make sure that the same partitions are used in the cv.
I really don't understand why I have to load spatialcv.Rdata here a third time...-->
<p>The output is a bias-reduced assessment of the model’s predictive performance.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># summary statistics of the 500 models</span>
<span class="kw">summary</span>(sp_cv$measures.test$auc)
<span class="co">#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </span>
<span class="co">#&gt;   0.692   0.757   0.792   0.788   0.838   0.872</span>
<span class="co"># mean AUROC of the 500 models</span>
<span class="kw">mean</span>(sp_cv$measures.test$auc)
<span class="co">#&gt; [1] 0.788</span></code></pre></div>
<p>To put it into perspective, we compare this result with that of a 100-repeated 5-fold non-spatial cross-validation (Figure <a href="spatial-cv.html#fig:boxplot-cv">13.5</a>; the code for the non-spatial cross-validation is not shown here but will be explored in the exercise section). As expected, the spatially cross-validated result yields lower AUROC values on average than the conventional cross-validation approach, underlining the over-optimistic predictive performance due to spatial autocorrelation of the latter.</p>
<div class="figure" style="text-align: center"><span id="fig:boxplot-cv"></span>
<img src="figures/boxplot-cv-1.png" alt="Boxplot showing the difference in AUROC values between spatial and conventional 100-repeated 5-fold cross-validation." width="576" />
<p class="caption">
Figure 13.5: Boxplot showing the difference in AUROC values between spatial and conventional 100-repeated 5-fold cross-validation.
</p>
</div>
</div>
<div id="svm" class="section level3">
<h3><span class="header-section-number">13.5.2</span> (Spatial) Tuning of machine-learning hyperparameters</h3>
<!-- exercise: assess predictive performance without using an inner fold -->
<p>In the beginning we have already distinguished the field of statistics from the field of machine learning. As a reminder we define machine learning here again with the words of <a href="https://machinelearningmastery.com/linear-regression-for-machine-learning/">Jason Brownlee</a>:</p>
<blockquote>
<p>Machine learning, more specifically the field of predictive modeling is primarily concerned with minimizing the error of a model or making the most accurate predictions possible, at the expense of explainability. In applied machine learning we will borrow, reuse and steal algorithms from many different fields, including statistics and use them towards these ends.</p>
</blockquote>
<p>In the previous section (section <a href="spatial-cv.html#glm">13.5.1</a>) we have used a GLM for predicting landslide susceptibility, in this section we will introduce the support vector machine (SVM) for the same purpose.</p>
<p>This means that parametric models such as (generalized) linear regression are also used in the field of machine learning when the aim is prediction.</p>
<p>Machine-learning differs from parametric models in that difference parameters and hyperparameters example: support vector machine and short intro what it is</p>
<p>SVM</p>
<blockquote>
<p>The idea of finding a hyperplane that separates the data as well as possible, while allowing some violations to this separation, seemed distinctly different from classical approaches for classification, such as logistic regression and linear discriminant analysis. Moreover, the idea of using a kernel to expand the feature space in order to accommodate non-linear class boundaries appeared to be a unique and valuable characteristic.</p>
</blockquote>
<p><span class="citation">(<span class="citeproc-not-found" data-reference-id="James"><strong>???</strong></span> et al., 2013)</span> Support vector machines always require tuning to find the optimal hyperparameters. Some software packages/implementations come with an automated tuning. which is usually based on random sampling. This is useful in the case of non-spatial data but of less use in the case of spatial data where spatial tuning should be preferred. Therefore, we will make sure to replace automated by spatial hyperparameter tuning. Without going into too much detail SVM</p>
<p>Setting up the <strong>mlr</strong> building blocks follows the exact same procedure introduced in the previous section (section <a href="spatial-cv.html#glm">13.5.1</a>), i.e. we define a task, a learner and a resampling strategy. The task remains the same, hence we can use the one already defined in the previous section (section <a href="spatial-cv.html#glm">13.5.1</a>), namely <code>task</code>. However, we have to define a new learner since we are going to use a SVM. So let us find out which SVM functions are available in the <strong>mlr</strong> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lrns =<span class="st"> </span>mlr::<span class="kw">listLearners</span>(task)
lrns[<span class="kw">grep</span>(<span class="st">&quot;svm&quot;</span>, lrns$class), ]
dplyr::<span class="kw">select</span>(lrns, class, name, package) 
<span class="co">#&gt;            class                                 name short.name package</span>
<span class="co">#&gt; 6   classif.ksvm              Support Vector Machines       ksvm kernlab</span>
<span class="co">#&gt; 9  classif.lssvm Least Squares Support Vector Machine      lssvm kernlab</span>
<span class="co">#&gt; 17   classif.svm     Support Vector Machines (libsvm)        svm   e1071</span></code></pre></div>
<p>We will use <code>ksvm()</code> from the kernlab package. To allow for non-linear relationships we use the radial basis function (or Gaussian) kernel which is also the default of <code>ksvm()</code> and probably the most popular SVM kernel in general.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lrn_ksvm =<span class="st"> </span><span class="kw">makeLearner</span>(<span class="st">&quot;classif.ksvm&quot;</span>,
                        <span class="dt">predict.type =</span> <span class="st">&quot;prob&quot;</span>,
                        <span class="dt">kernel =</span> <span class="st">&quot;rbfdot&quot;</span>)</code></pre></div>
<p>Hence, the only thing left to do is to specify a resampling strategy. Again we will use a 100-repeated 5-fold spatial CV.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># outer resampling loop</span>
outer =<span class="st"> </span><span class="kw">makeResampleDesc</span>(<span class="st">&quot;SpRepCV&quot;</span>, <span class="dt">folds =</span> <span class="dv">5</span>, <span class="dt">reps =</span> <span class="dv">100</span>)</code></pre></div>
<p>So far, this is exactly the same as we have done when using the GLM, however, now we need to additionally tune the SVM hyperparameters. Using the same data for the performance assessment and the tuning would potentially lead to overoptimistic results <span class="citation">(<span class="citeproc-not-found" data-reference-id="cawley_on_2010"><strong>???</strong></span>)</span>. To avoid this we will use a nested spatial CV.</p>
<div class="figure" style="text-align: center"><span id="fig:inner-outer"></span>
<img src="figures/13_cv.png" alt="Visual representation of inner and outer folds in spatial and non-spatial cross-validation. Permission for reproducing the figure was kindly granted by Patrick Schratz [@schratz_performance_nodate]." width="500" />
<p class="caption">
Figure 13.6: Visual representation of inner and outer folds in spatial and non-spatial cross-validation. Permission for reproducing the figure was kindly granted by Patrick Schratz <span class="citation">(Schratz et al. <a href="#ref-schratz_performance_nodate">2018</a>)</span>.
</p>
</div>
<p>This means that we split each fold again into five spatially disjoint subfolds which are used to determine the optimal hyperparameters (<code>inner</code> object in the code chunk below; see Figure <a href="spatial-cv.html#fig:inner-outer">13.6</a> for a visual representation). To find the optimal hyperparameter combination we here fit 50 models in each of these subfolds with randomly selected hyperparameter values (<code>ctrl</code> object in the code chunk below). Additionally, we restrict the randomly chosen values to a predefined tuning space (<code>ps</code> object). The latter was chosen with values recommended in the literature <span class="citation">(Schratz et al. <a href="#ref-schratz_performance_nodate">2018</a>)</span>.</p>
<!--
Questions Pat:
- why not using e1071 svm -> inner hyperparameter tuning also possible I guess...
- explanation correct?
- trafo-function?
- 125,000 models
- mc.set.seed = TRUE -> make sure that the partitioning remains the same in each thread
- can I compare the mean AUROC of the GLM and the SVM when using the same seed? Or is seeding not strictly necessary? I mean, ok, the partitions vary a bit but overall...
-->
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># five spatially disjoint partitions</span>
inner =<span class="st"> </span><span class="kw">makeResampleDesc</span>(<span class="st">&quot;SpCV&quot;</span>, <span class="dt">iters =</span> <span class="dv">5</span>)
<span class="co"># use 50 randomly selected hyperparameters</span>
ctrl =<span class="st"> </span><span class="kw">makeTuneControlRandom</span>(<span class="dt">maxit =</span> <span class="dv">50</span>)
<span class="co"># define the outer limits of the randomly selected hyperparameters</span>
ps =<span class="st"> </span><span class="kw">makeParamSet</span>(
  <span class="kw">makeNumericParam</span>(<span class="st">&quot;C&quot;</span>, <span class="dt">lower =</span> -<span class="dv">12</span>, <span class="dt">upper =</span> <span class="dv">15</span>, <span class="dt">trafo =</span> function(x) <span class="dv">2</span>^x),
  <span class="kw">makeNumericParam</span>(<span class="st">&quot;sigma&quot;</span>, <span class="dt">lower =</span> -<span class="dv">15</span>, <span class="dt">upper =</span> <span class="dv">6</span>, <span class="dt">trafo =</span> function(x) <span class="dv">2</span>^x)
  )</code></pre></div>
<p>Finally, we modify our learner <code>lrn_ksvm</code> in accordance with all the parameters defining the inner hyperparameter through a wrapper function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">wrapped_lrn_ksvm =<span class="st"> </span><span class="kw">makeTuneWrapper</span>(<span class="dt">learner =</span> lrn_ksvm, 
                                   <span class="dt">resampling =</span> inner,
                                   <span class="dt">par.set =</span> ps,
                                   <span class="dt">control =</span> ctrl, 
                                   <span class="dt">show.info =</span> <span class="ot">TRUE</span>,
                                   <span class="dt">measures =</span> mlr::auc)</code></pre></div>
<p>Overall, this set up implies that we ask R to fit 250 models to determine the optimal hyperparameters which are then used for the performance assessment in the first fold of the outer resampling loop. We have to repeat this five times for each fold in the outer fold which leads to 250 * 5 models for one repetition in the outer loop. Since we are requesting 100 repetitions this leads to a total of 125,000 models. This is computationally quite demanding even with the small dataset used here. So before starting the actual resampling it would be wise to reduce runtime with the help of a parallelization approach. In general, multicore processing is easier on Linux than on Windows systems. In fact, cloud computing is usually done and developed on Linux servers. Therefore, we will present how to do nested cross-validatation using a parallelization approach working only under Linux-based systems.<a href="#fn67" class="footnoteRef" id="fnref67"><sup>67</sup></a>. Windows users have at least two options:</p>
<ol style="list-style-type: decimal">
<li>Run the resampling without parallelizing it though this might take a while.</li>
<li>Install a virtual machine (e.g. <a href="https://www.virtualbox.org/">Oracle VirtualBox</a>) to reproduce the parallelization code.</li>
</ol>
<p>Before starting the parallelization, we make sure that the processing continues even if one of the models throws an error by setting <code>on.learner.error</code> to <code>warn</code>. To inspect errored models after the processing has completed we dump them. This is quite handy since you really want to avoid that the processing stops after you have run a server at full capacity for several days just because one model cannot be fitted.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">configureMlr</span>(<span class="dt">on.learner.error =</span> <span class="st">&quot;warn&quot;</span>, <span class="dt">on.error.dump =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p>First, we set the <code>mode</code> to <code>multicore</code> which will use <code>mclapply()</code> in the background on a single machine.<a href="#fn68" class="footnoteRef" id="fnref68"><sup>68</sup></a> <code>level</code> defines the level where to enable the parallelization with <code>mlr.tuneParams</code> determing that the inner resampling fold should be parallelized, i.e. the optimal hyperparameter tuning (see lower part of Figure <a href="spatial-cv.html#fig:inner-outer">13.6</a>; and use parallelMap::parallelGetRegisteredLevels() for supported levels as well as the <strong>mlr</strong> <a href="https://mlr-org.github.io/mlr-tutorial/release/html/parallelization/index.html#parallelization-levels">parallelization tutorial</a>). Probably we are not the only ones using the server, therefore we are friendly and will use only half of its cores which in our case corresponds to 24 (<code>cpus</code> parameter). To make sure that the same partitions are used in each parallelized thread, we set <code>mc.set.seed</code> to <code>TRUE</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># parallelize the tuning, i.e. the inner fold</span>
<span class="kw">parallelStart</span>(<span class="dt">mode =</span> <span class="st">&quot;multicore&quot;</span>, 
              <span class="dt">level =</span> <span class="st">&quot;mlr.tuneParams&quot;</span>, 
              <span class="co"># just use half of the available cores</span>
              <span class="dt">cpus =</span> <span class="kw">round</span>(parallel::<span class="kw">detectCores</span>() /<span class="st"> </span><span class="dv">2</span>),
              <span class="dt">mc.set.seed =</span> <span class="ot">TRUE</span>) </code></pre></div>
<p>Finally, we are all set up for conducting the nested spatial CV. Specifying the <code>resample()</code> parameters follows the exact same procedure as presented when using a GLM with the only difference that we additionally tell it to give back the hyperparameter tuning results (<code>extract</code> parameter). After the processing, it is good practice to explicitly stop the parallelization which is exactly what <code>parallelStop()</code> is doing. Of course, after a long processing it is a good idea to save your results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">12345</span>)
result =<span class="st"> </span>mlr::<span class="kw">resample</span>(<span class="dt">learner =</span> wrapped_lrn_ksvm, 
                       <span class="dt">task =</span> task,
                       <span class="dt">resampling =</span> outer,
                       <span class="dt">extract =</span> getTuneResult,
                       <span class="dt">measures =</span> mlr::auc)
<span class="co"># stop parallelization</span>
<span class="kw">parallelStop</span>()
<span class="co"># save your result, e.g.:</span>
<span class="co"># saveRDS(resa_svm_spatial, &quot;svm_sp_sp_rbf_50it.rda&quot;)</span></code></pre></div>
<p>Running 125,000 models using 24 cores took more than 37 minutes.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Exploring the results</span>
<span class="co"># run time in minutes</span>
<span class="kw">round</span>(result$runtime /<span class="st"> </span><span class="dv">60</span>, <span class="dv">2</span>)
<span class="co">#&gt; [1] 37.4</span></code></pre></div>
<p>Naturally, even more important than the runtime is the final aggregated AUROC, i.e. the model’s ability to discriminate the two classes.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># final aggregated AUROC </span>
result$aggr
<span class="co">#&gt; auc.test.mean </span>
<span class="co">#&gt;         0.758</span>
<span class="co"># same as</span>
<span class="kw">mean</span>(result$measures.test$auc)
<span class="co">#&gt; [1] 0.758</span></code></pre></div>
<p>So it appears that the GLM is slightly better than the SVM in this specific case. Finally, we can have a look at the results of the found optimal hyperparameters for each outer loop iteration. Here, we just have a short glance at the optimal hyperparameters combination and the result which was obtained with them in the first iteration of the outer loop</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># used hyperparameters for the outer fold, i.e. the best combination out of 50 *</span>
<span class="co"># 5 models</span>
result$extract[[<span class="dv">1</span>]]
<span class="co">#&gt; Tune result:</span>
<span class="co">#&gt; Op. pars: C=0.458; sigma=0.023</span>
<span class="co">#&gt; auc.test.mean=0.823</span>
<span class="co"># and here one can observe that the AUC of the tuning data is usually higher</span>
<span class="co"># than for the model on the outer fold</span>
result$measures.test[<span class="dv">1</span>, ]
<span class="co">#&gt;   iter   auc</span>
<span class="co">#&gt; 1    1 0.799</span></code></pre></div>
</div>
</div>
<div id="conclusions" class="section level2">
<h2><span class="header-section-number">13.6</span> Conclusions</h2>
<p>Resampling methods are a crucial part of a modern data scientist’s toolbox <span class="citation">(James et al. <a href="#ref-james_introduction_2013">2013</a>)</span>. In this chapter we used cross-validation to assess a model’s predictive performance. Spatial data is statistically often not independent due to spatial autocorrelation, which violates a fundamental assumption of cross-validation. Therefore, we introduced spatial CV, which reduces the bias introduced by spatial autocorrelation. The <strong>mlr</strong> package makes it easy to use (spatial) resampling techniques with many other statistical learning techniques including, of course, linear regression, but also semi-parametric models (e.g., generalized additive models) and machine learning techniques such as random forests, support vector machines or boosted regression trees <span class="citation">(Bischl et al. <a href="#ref-bischl_mlr:_2016">2016</a>; Schratz et al. <a href="#ref-schratz_performance_nodate">2018</a>)</span>. Machine learning algorithms often require the tuning of so-called hyperparameters. Naturally, computation time additionally increases with the size of the input data. To reduce computing time, <strong>mlr</strong> makes parallelization easy through various supported methods.</p>
<p>Finally, for more details please check out also the fantastic <strong>mlr</strong> online documentation:</p>
<ul>
<li><a href="https://mlr-org.github.io/mlr-tutorial/" class="uri">https://mlr-org.github.io/mlr-tutorial/</a></li>
<li><a href="https://github.com/mlr-org/mlr/wiki/user-2015-tutorial" class="uri">https://github.com/mlr-org/mlr/wiki/user-2015-tutorial</a></li>
</ul>
</div>
<div id="exercises-10" class="section level2">
<h2><span class="header-section-number">13.7</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li>Compute the terrain attributes slope, plan curvature, profile curvature and catchment area from <code>dem</code> (provided by <code>data(&quot;landslides&quot;, package = &quot;RSAGA&quot;)</code>) with the help of R-GIS bridges (see Chapter <a href="gis.html#gis">10</a>), and extract the values from the corresponding output rasters to the <code>landslides</code> dataframe (<code>data(landslides, package = &quot;RSAGA&quot;</code>)). Keep all landslide initation points and 175 randomly selected non-landslide points (see section <a href="spatial-cv.html#case-landslide">13.2</a>).</li>
<li>Use the derived terrain attributs rasters in combination with a GLM to make a spatial prediction map similar to Figure <a href="spatial-cv.html#fig:lsl-susc">13.2</a>.</li>
<li>Compute a 100-repeated 5-fold non-spatial cross-validation and spatial CV based on the GLM learner and compare the AUROC values from both resampling strategies with the help of boxplots (see Figure <a href="spatial-cv.html#fig:boxplot-cv">13.5</a>). Hint: You need to specify a non-spatial task and a non-spatial resampling strategy. Before running the spatial cross-validation for both tasks set a seed to make sure that both use the same partitions which in turn guarantees comparability.</li>
<li>Model landslide susceptibility using a quadratic discriminant analysis <span class="citation">(QDA, James et al. <a href="#ref-james_introduction_2013">2013</a>)</span>. Assess the predictive performance (AUROC) of the QDA. What is the a difference between the spatially cross-validated mean AUROC value of the QDA and the GLM? Hint: Before running the spatial cross-validation for both learners set a seed to make sure that both use the same partitions which in turn guarantees comparability.</li>
</ol>
<!--
hyperparameter tuning:
The training data is again partitioned into 5 folds but only once.
Now each fold is used once as a test set, and the remaining training data is used to find the optimal hyperparameter tuning via a random search with 50 (or whatever number) iterations -> 250 iterations to find the optimal hyperparameter combination. 
This combination serves as input for the model in the outer level.

Hyperparameters are always tuned in mlr in an inner loop (I suppose). 
But why do we need the inner tuning.
Well, otherwise we would tune our hyperparameters on the test set of the outer loop, and this is like taking a sneak preview.
-->

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-zuur_mixed_2009">
<p>Zuur, Alain, Elena N. Ieno, Neil Walker, Anatoly A. Saveliev, and Graham M. Smith. 2009. <em>Mixed Effects Models and Extensions in Ecology with R</em>. Statistics for Biology and Health. New York: Springer-Verlag. <a href="//www.springer.com/de/book/9780387874579" class="uri">//www.springer.com/de/book/9780387874579</a>.</p>
</div>
<div id="ref-james_introduction_2013">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani, eds. 2013. <em>An Introduction to Statistical Learning: With Applications in R</em>. Springer Texts in Statistics 103. New York: Springer.</p>
</div>
<div id="ref-schratz_performance_nodate">
<p>Schratz, Patrick, J. Muenchow, Eugenia Iturritxa, Jakob Richter, and A. Brenning. 2018. “Performance Evaluation and Hyperparameter Tuning of Statistical and Machine-Learning Models Using Spatial Data.” <em>Tba</em>.</p>
</div>
<div id="ref-muenchow_geomorphic_2012">
<p>Muenchow, J., A. Brenning, and M. Richter. 2012. “Geomorphic Process Rates of Landslides Along a Humidity Gradient in the Tropical Andes.” <em>Geomorphology</em> 139-140 (February): 271–84. doi:<a href="https://doi.org/10.1016/j.geomorph.2011.10.029">10.1016/j.geomorph.2011.10.029</a>.</p>
</div>
<div id="ref-bivand_applied_2013">
<p>Bivand, Roger S., Edzer Pebesma, and Virgilio Gómez-Rubio. 2013. <em>Applied Spatial Data Analysis with R</em>. 2nd ed. 2013 edition. New York: Springer.</p>
</div>
<div id="ref-zuur_beginners_2017">
<p>Zuur, Alain F., Elena N. Ieno, Anatoly A. Saveliev, and Alain F. Zuur. 2017. <em>Beginner’s Guide to Spatial, Temporal and Spatial-Temporal Ecological Data Analysis with R-INLA</em>. Vol. Volume 1: Using GLM and GLMM. 2 vols. Newburgh, United Kingdom: Highland Statistics Ltd.</p>
</div>
<div id="ref-blangiardo_spatial_2015">
<p>Blangiardo, Marta, and Michela Cameletti. 2015. <em>Spatial and Spatio-Temporal Bayesian Models with R-INLA</em>. Chichester, UK: John Wiley &amp; Sons, Ltd. doi:<a href="https://doi.org/10.1002/9781118950203">10.1002/9781118950203</a>.</p>
</div>
<div id="ref-miller_toblers_2004">
<p>Miller, Harvey J. 2004. “Tobler’s First Law and Spatial Analysis.” <em>Annals of the Association of American Geographers</em> 94 (2).</p>
</div>
<div id="ref-brenning_spatial_2012">
<p>Brenning, Alexander. 2012b. “Spatial Cross-Validation and Bootstrap for the Assessment of Prediction Rules in Remote Sensing: The R Package Sperrorest.” In, 5372–5. IEEE. doi:<a href="https://doi.org/10.1109/IGARSS.2012.6352393">10.1109/IGARSS.2012.6352393</a>.</p>
</div>
<div id="ref-bischl_mlr:_2016">
<p>Bischl, Bernd, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob Richter, Erich Studerus, Giuseppe Casalicchio, and Zachary M. Jones. 2016. “Mlr: Machine Learning in R.” <em>Journal of Machine Learning Research</em> 17 (170): 1–5. <a href="http://jmlr.org/papers/v17/15-066.html" class="uri">http://jmlr.org/papers/v17/15-066.html</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="60">
<li id="fn60"><p>Readers who are in need of refreshing their regression skills might have a look at <span class="citation">Zuur et al. (<a href="#ref-zuur_mixed_2009">2009</a>)</span> and <span class="citation">James et al. (<a href="#ref-james_introduction_2013">2013</a>)</span>, respectively.<a href="spatial-cv.html#fnref60">↩</a></p></li>
<li id="fn61"><p>These correlation structures can also be included in <code>MASS::glmmPQL()</code> and <code>mgcv::gamm()</code>.<a href="spatial-cv.html#fnref61">↩</a></p></li>
<li id="fn62"><p>Note that for spatial predictions one would usually use the population intercept.<a href="spatial-cv.html#fnref62">↩</a></p></li>
<li id="fn63"><p>As pointed out in the beginning we will solely focus on supervised learning techniques in this chapter.<a href="spatial-cv.html#fnref63">↩</a></p></li>
<li id="fn64"><p>In the case of a regression problem, we would use <code>makeRegrTask()</code>. Type <code>?makeClassifTask</code> to find out about all available modeling tasks.<a href="spatial-cv.html#fnref64">↩</a></p></li>
<li id="fn65"><p>Note that this corresponds to <code>type = response</code> in <code>predict.glm</code>.<a href="spatial-cv.html#fnref65">↩</a></p></li>
<li id="fn66"><p>The <strong>caret</strong> package is another umbrella-package <span class="citation">(Kuhn and Johnson <a href="#ref-kuhn_applied_2013">2013</a>)</span> for streamlined modeling in R, however, so far it does not provide spatial CV which is why we refrain from using it for spatial data.<a href="spatial-cv.html#fnref66">↩</a></p></li>
<li id="fn67"><p>Note also that the <code>mc.set.seeds</code> parameter used later on is only available for Linux machines<a href="spatial-cv.html#fnref67">↩</a></p></li>
<li id="fn68"><p>Check out <code>?parallelStart</code> for further modes and the <strong>parallelMap</strong> <a href="https://github.com/berndbischl/parallelMap">github page</a> for more information on the unified interface to popular parallelization back-ends. Note also that <code>mclapply()</code> only supports multicore processing on Linux-based machines.<a href="spatial-cv.html#fnref68">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="algorithms-and-functions-for-geocomputation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/Robinlovelace/geocompr/edit/master/13-spatial-cv.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
