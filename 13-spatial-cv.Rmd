# Spatial cross-validation for machine learning {#spatial-cv}

## Prerequisites {-}

This chapter requires a strong grasp of spatial data analysis and processing, covered in chapters \@ref(spatial-class) to \@ref(transform).
You should also be familiar with linear regression, its generalized extensions and machine learning [e.g. @zuur_mixed_2009;@james_introduction_2013].

The chapter uses the following packages:

```{r, message = FALSE} 
library(mlr)
library(pROC)
library(raster)
library(RSAGA)
library(sf)
library(tidyverse)
library(tibble)
```

<!-- This sounds a bit strange. Also a single bullet point here looks a bit lost? -->
- Required data will be downloaded in due course.

## Introduction

Section \@ref(software-for-geocomputation) mentioned several programming languages suitable for command-line based geocomputation.
<!-- unparalleled = outstanding?  -->
The advantages of geocomputation with R were discussed, including its unparalleled statistical power.
This chapter makes use of some of this statistical power, by demonstrating methods for predictive mapping by means of statistical learning [@james_introduction_2013].
The main focus, however, is the use of spatial cross-validation (or 'spatial CV' in short, a term we will define shortly) to assess model performance and reduce spatial bias.
<!-- I think this is confusing as it sounds that you use spatial cv to model/predict something. However, its only purpose is performance estimation. "CV is an excellent method to get a robust estimate of the performance of an algorithm on a data set. At the time of writing, Cv approaches tailored to spatial data are best supported in R than any other language." -->
Spatial CV is an excellent example of using statistical methods to model spatial data and, at the time of writing, the technique is better supported in R than any other language.

Statistical learning comprises a large suite of techniques for understanding data.
Statistical learning can be roughly grouped into supervised and unsupervised techniques, both of which are used throughout a vast range of disciplines including economics, physics, medicine, biology, ecology and geography [@james_introduction_2013].
In this chapter we will focus on supervised techniques, i.e., we have a response variable, in our case this will be a binary one (landslide vs. non-landslide occurrence) but could be also a numeric (pH value), an integer (species richness) or a categorical variable (land use).
Supervised techniques model the relationship between the response variable and various predictors.
For this we can use techniques from the field of statistics or from the field of machine learning.
<!-- This sounds like that statistical models should not be used for prediction at all. Maybe write sth like "the possibility of conducting statistical inference or maximization of predictive accuracy" -->
Which to use depends on the aim: statistical inference or prediction.
(Semi-)Parametric regression techniques are especially useful if the aim is statistical inference, i.e. if we are interested in a predictor's significance, its importance for a specific model and to explain relationships between response and predictors.
To trust the p-values and standard errors of such models we need to perform a thorough model validation testing if one or several of the underlying model assumptions (heterogeneity, independence, etc.) have been violated [@zuur_mixed_2009].
<!-- This again sounds like statistical models are not aimed for prediction at all (because you use "by contrast")-->
By contrast, machine learning aims at predictions and is especially appealing due its lack of assumptions.
<!-- Usually ML > SL, so we would need to formulate it as "SL has the ability to be at least par with ML". I would not write about statistical inference and predictive performance in one sentence as they are unrelated here. Stay with one: "Although stat. inf. is impossible, some ML models provide the possibility to calcuilate variable importance" -->
Though statistical inference is impossible [@james_introduction_2013], various studies have shown that machine learning is at least at par with regression techniques regarding predictive performance [e.g., @schratz_performance_nodate]. 
<!-- machine learning has even gained in popularity "due to its ability to scale to large data and ignore the assumption of uncorrelated predictor variables" -->
Naturally, with the advent of big data, machine learning has even gained in popularity since frequently the underlying relationship between variables is less important than the prediction.
Think for instance of future customer behavior, recommendation services (music, movies, what to buy next), face recognition, autonomous driving, classifying e-mails as spam and predictive maintenance (infrastructure, industry) to name just a few.
<!-- "borrow sounds wrong here since we do not borrow something, especially not from "statitics", This sounds like it would be a different 'site'? Simply something like "Fortunately, we can use machine-learning regression techniques to deal with such lrage data" -->
Note that we can borrow regression techniques from statistics for machine learning when the aim is prediction.
In this case we do not have too worry too much about possible model misspecifications since we explicitly do not want to do statistical inference.
In this chapter the aim will be the (spatial) prediction of landslide susceptibility. 
<!-- Somethings missing after the GLM reference -->
We will start out with a Generalized Linear Model (GLM) which is assumed to be familiar to most readers^[Readers who are in need of refreshing their regression skills might have a look at @zuur_mixed_2009 and @james_introduction_2013, respectively.], before moving on to using a machine-learning algorithm, in this case a support vector machine.
Instead of focusing on model specifications we will tackle the speciality of geographic data in a modeling context, specifically on performance estimates received using (spatial) CV.

CV determines a model's ability to generalize to new data.
To achieve this, CV splits a dataset (repeatedly) into test and training sets.
It uses the training data to fit the model, and checks its performance when predicting to the test data.
Basically, cross-validation helps to detect overfitting since a model that fits the training data too closely (including the data's noise and random fluctuations) will have a bad prediction performance on the test data.
The basic requirement for this is that the test data consists of observations that the model has not seen before.
CV achieves this by splitting the data randomly into test and training set. 
However, randomly splitting spatial data results in the fact that training points are directly located next to test points.
<!-- This probably needs to be reformulated again -->
Since points close to each other are more similar compared to points further away (this is called spatial autocorrelation), test and training datasets might be independent in theory but not in practice.
Hence, many observations from the test set are highly similar to the training data.
<!-- "overfitting" is only valid if the diff between train and test is high -->
The consequence is that cross-validation cannot detect possible overfitting because the model will achieve a very good performanc e on the test set because its highly similar to the training data.
Spatial CV is able to solve this problem and will be main topic of this chapter.

## Case study: Landslide susceptibility {#case-landslide}

To introduce spatial CV by example, we will use a landslide dataset from Southern Ecuador (Figure \@ref(fig:lsl-map)).
For a detailed description of the dataset and the study area please refer to @muenchow_geomorphic_2012.
One can find a subset of the corresponding data in the **RSAGA** package.
The following command loads three datasets, a `data.frame` named `landslides`, a `list` named `dem`, and an `sf`-object named `study_area`.

```{r}
data("landslides", package = "RSAGA")
```

`landslides` contains a factor column `lslpts` where `TRUE` corresponds to an observed landslide initiation point and `FALSE` to points where no landsliding occurred. 
Columns `x` and `y` contain the corresponding coordinates.
The landslide initiation point is located in the scarp of a landslide polygon.
The coordinates for the non-landslide points were sampled randomly with the restriction to fall outside of the slightly buffered landslide polygons.
`summary(landslides$lslpts)` tells us that 175 landslide points and 1360 non-landslide points are available.
To make the ratio between landslide and non-landslide points more balanced, we randomly sample 175 from the 1360 non-landslide points.

```{r, eval = FALSE}
# select non-landslide points
non = filter(landslides, lslpts == FALSE)
# select landslide points
lsl_pts = filter(landslides, lslpts == TRUE)
# randomly select 175 non-landslide points
ind = sample(1:nrow(non), nrow(lsl_pts))
# rowbind randomly selected non-landslide points and 
# landslide points
lsl = rbind(non[ind, ], lsl_pts)
```

`dem` is in fact a digital elevation model and consists of two list elements with the first being a raster header and the second being a matrix containing the altitudinal values.
To transform this list into a `raster` object, we can write:

```{r, eval = FALSE}
dem = 
  raster(dem$data, 
         crs = dem$header$proj4string,
         xmn = dem$header$xllcorner, 
         xmx = dem$header$xllcorner + 
           dem$header$ncols * dem$header$cellsize,
         ymn = dem$header$yllcorner,
         ymx = dem$header$yllcorner + 
           dem$header$nrows * dem$header$cellsize)
```

To model the probability for landslide occurrence, we need some predictors.
We will use selected terrain attributes frequently associated with landsliding [@muenchow_geomorphic_2012], all of which can be computed from the provided digital elevation model (`dem`) using R-GIS bridges (see Chapter \@ref(gis)).
We leave it as an exercise to the reader to compute the terrain attribute rasters and extract the corresponding values to our landslide/non-landslide dataframe (see also exercises).
The first three rows of the resulting dataframe (still named `lsl`) excluding the coordinates could look like this:
<!-- has anybody an idea why I have to run the following code chunk two times to make it work when rendering the book with `bookdown::render_book()`?-->
```{r, echo=FALSE}
load("extdata/spatialcv.Rdata")
```

```{r, echo=FALSE}
load("extdata/spatialcv.Rdata")
```

```{r}
dplyr::select(lsl, -x, -y) %>%
  head(3)
```

<!-- reviewed until here -->

The added columns are:

- `slope`: slope angle (°)
- `cplan`: plan curvature (rad m^−1^) expressing the convergence or divergence of a slope and thus water flow.
- `cprof`: profile curvature (rad m^-1^) as a measure of flow acceleration, also known as downslope change in slope angle 
- `elev`: elevation (m a.s.l.) as the representation of different altitudinal zones of vegetation and precipitation in the study area.
- `log_carea`: the decadic logarithm of the catchment area (log m^2^) representing the amount of water flowing towards a location.

```{r lsl-map, echo=FALSE, fig.cap="Landslide initiation points (red) and points unaffected by landsliding (blue) in Southern Ecuador. CRS: UTM zone 17S (EPSG: 32717)."}
library(tmap)
lsl_sf = st_as_sf(lsl, coords = c("x", "y"), crs = 32717)
hs = hillShade(ta$slope * pi / 180, terrain(ta$elev, opt = "aspect"))
rect = tmaptools::bb_poly(hs)
bbx = tmaptools::bb(hs, xlim = c(-0.02, 1), ylim = c(-0.02, 1), relative = TRUE)
# randomly sample 20%
# ind = sample(1:nrow(lsl_sf), round(nrow(lsl_sf) * 0.2))
# sam = lsl_sf[ind, ]

tm_shape(hs, bbox = bbx) +
	tm_grid(col = "black", n.x = 1, n.y = 1, labels.inside.frame = FALSE,
	        labels.rot = c(0, 90)) +
	tm_raster(palette = gray(0:100 / 100), n = 100, legend.show = FALSE) +
	tm_shape(ta$elev) +
	tm_raster(alpha = 0.5, palette = terrain.colors(10),
	          auto.palette.mapping = FALSE, legend.show = FALSE) +
	tm_shape(lsl_sf) + 
	tm_bubbles("lslpts", size = 0.5, palette = "-RdYlBu") +
#   tm_shape(sam) +
#   tm_bubbles(border.col = "gold", border.lwd = 2, alpha = 0, size = 0.5) +
  qtm(rect, fill = NULL) +
	tm_layout(outer.margins = c(0.04, 0.04, 0.02, 0.02), frame = FALSE)
```


## Conventional modeling approach in R {#conventional-model}
Later on we will introduce the **mlr** package, an umbrella-package providing a unified interface to a plethora of algorithms. 
Before doing so, it is worth taking a look at the conventional modeling interface in R.
This way we introduce statistical supervised modeling in R which provides the required skill set for doing spatial CV and additionally contributes to a better grasp on the **mlr** approach introduced later on.
Usually, we model the response variable as a function of predictors. 
Therefore, most algorithms implementations in R such as `lm`, `glm` and many more use the so-called formula interface.
We put this into practice by modeling the landslide occurrence as a function of terrain attributes.
Since our response (landslide occurrence) is binary, we use a generalized linear model with a binomial family and a logit link function instead of a simple linear model which does not allow for other response distributions besides Gaussian.

```{r, eval = TRUE}
# meist wird einfach "binomial" geschrieben anstatt binomial()
fit = glm(lslpts ~ slope + cplan + cprof + elev + log_carea, 
          data = lsl, family = binomial())
# the same as:
# fit = glm(lslpts ~ ., data = select(lsl, -x, -y))
fit
```

Subsequently, we can use the estimated model coefficients for predictions.
The generic S3 `predict()` function automatically chooses the respective function based on the fitted model.
When setting `type = response` it returns the predicted probabilities (of landslide occurrence) for each observation in `lsl` (see `?predict.glm`).


```{r}
head(predict(object = fit, type = "response"))
```

We can also predict spatially by applying the coefficients to our predictor rasters. 
We could do this manually but can also use **raster**'s `predict()` function.
This function also expects the fitted model as input as well as a raster stack with the predictors exactly named as in the fitted model (Figure \@ref(fig:lsl-susc)).

```{r}
# loading among others ta, a raster stack containing the predictors
load("extdata/spatialcv.Rdata")
pred = raster::predict(object = ta, model = fit,
                       type = "response")
```

Ich würde maps immer lat/lon darstellen, UTM coords find ich immer undescriptive.

```{r lsl-susc, echo = FALSE, fig.cap="Spatial prediction of landslide susceptibility using a  GLM. CRS: UTM zone 17S (EPSG: 32717).", warning=FALSE}
# white raster to only plot the axis ticks, otherwise gridlines would be visible
tm_shape(hs, bbox = bbx) +
  tm_grid(col = "black", n.x = 1, n.y = 1, labels.inside.frame = FALSE,
          labels.rot = c(0, 90)) +
  tm_raster(palette = "white", legend.show = FALSE) +
  # hillshade
  tm_shape(mask(hs, study_area), bbox = bbx) +
	tm_raster(palette = gray(0:100 / 100), n = 100, legend.show = FALSE) +
	# prediction raster
  tm_shape(mask(pred, study_area)) +
	tm_raster(alpha = 0.5, palette = RColorBrewer::brewer.pal(name = "Reds", 6),
	          auto.palette.mapping = FALSE, legend.show = TRUE,
	          title = "Susceptibility\nprobability") +
	# rectangle and outer margins
  qtm(rect, fill = NULL) +
	tm_layout(outer.margins = c(0.04, 0.04, 0.02, 0.02), frame = FALSE,
	          legend.position = c("left", "bottom"),
	          legend.title.size = 0.9)

```

Here, when making predictions we neglect spatial autocorrelation since we assume that on average the predictive accuracy remains the same with or without spatial autocorrelation structures.
# Sp. autocor. structures can afaik only included during model fitting?
However, it is possible to include spatial autocorrelation structures into models as well as into the predictions.
This is, however, beyond the scope of this book.
Nevertheless, we give the interested reader some pointers where to look it up.
There are three main directions:

1. The predictions of universal kriging are the predictions of a simple linear model plus the kriged model's residuals, i.e. spatially interpolated residuals [@bivand_applied_2013]. 
# When reading probably most will oversee the foonote here. I would put the MASS and mgcv references into the text directly
2. Adding a spatial correlation (dependency) structure to a generalized least squares model  [`nlme::gls()`; @zuur_mixed_2009; @zuur_beginners_2017].  ^[These correlation structures can also be included in `MASS::glmmPQL()` and `mgcv::gamm()`.]
# if glmmPQL and gamm oben are named in the previous point, one may think that they are not mixed models as the point comes next? just a thought
3. Finally, there are mixed-effect modeling approaches.
Basically, a random effect imposes a dependency structure on the response variable which in turn allows for observations of one class to be more similar to each other than to those of another class [@zuur_mixed_2009]. 
Classes can be, for example, bee hives, owl nests, vegetation transects or an altitudinal stratification.
This mixed modeling approach assumes normal and independent distributed random intercepts.^[Note that for spatial predictions one would usually use the population intercept.]
This can even be extended by using a random intercept that is normal and spatially dependent.
For this, however, you will have to resort most likely to Bayesian modeling approaches since frequentist software tools are rather limited in this respect especially for more complex models [@blangiardo_spatial_2015; @zuur_beginners_2017]. 

Spatial predictions are one very important outcome of a model.
Even more important is how good the model is at making them since the most beautiful prediction map is useless if a model's predictive performance is bad.
The most popular measure to assess the predictive performance of a binomial model is the Area Under the Receiver Operator Characteristic Curve (AUROC).
This is a value between 0.5 and 1.0 with 0.5 indicating no and 1.0 indicating a perfect discrimination of the two classes. 
Thus, the higher the AUROC the better is our model at making predictions.
In the following we compute the receiver operator characteristic with the help of `roc()` by providing it with the response variable and the predicted values. 
`auc()` returns the area under the curve.

```{r, message=FALSE}
pROC::auc(pROC::roc(lsl$lslpts, fitted(fit)))
```

An AUROC of `r round(pROC::auc(pROC::roc(lsl$lslpts, fitted(fit))), 2)` represents a good fit.
However, this is an overoptimistic estimation since we have computed it on the complete dataset. 
To derive a biased-reduced assessment we have to use cross-validation and in the case of spatial data we will have to make use of spatial CV.

## Introduction to (spatial) cross-validation {#intro-cv} 

Cross-validation belongs to the family of resampling methods [@james_introduction_2013].
The basic idea is to split (repeatedly) a dataset into training and test sets whereby the training data is used to fit a model which then is applied to the test set.
Comparing the predicted values with the known response values from the test set (using a performance measure such as the AUROC in the binomial case) gives a bias-reduced assessment of the model's capability to generalize the learned relationship to independent data.
For example, a 100-repeated 5-fold cross-validation means to randomly split the data into five partitions (folds) with each fold being used once as a test set (see upper row of Figure \@ref(fig:partitioning)). 
This guarantees that each observation is used once in one of the test sets, and requires the fitting of five models.
Subsequently, this procedure is repeated 100 times.
Of course, the data splitting will differ in each repetition.
# if the error is calc. on the fold-level. most often its calc. on the repetition level. maybe worth noting.
Overall, this sums up to 500 models whereas the mean performance measure (AUROC) of all models is the model's overall prediction power.

However, geographic data is special.
Remember that the first law of geography states that points close to each other tend to be, on average, more similar compared to points further away (@miller_toblers_2004; Chapter \@ref(transport)).
This means these points are not statistically independent because training and test points in conventional cross-validation are often too close to each other (see first row of \@ref(fig:partitioning)).
# this should be rephrased, maybe: Using this information during model fitting is like cheating in a test, i.e. testing the model on observations that are almost identical to the ones it was trained on.
Using this information during model fitting is like a sneak preview, i.e. using information that should be unavailable to the training dataset.
# "folds" only for the repetition split, "partitions" or "subsets" for splitting within a fold
To alleviate this problem, we should make use of spatial partitioning which splits the observations into spatially disjoint subsets (using the observations' coordinates in a *k*-means clustering; @brenning_spatial_2012; second row of Figure \@ref(fig:partitioning)).
The partitioning strategy is **the** distinguishing feature between spatial and conventional cross-validation.
Everything else remains exactly the same.
# I always write "overfitting"
As a result spatial CV leads to a bias-reduced assessment of a model's predictive performance, and hence helps to avoid over-fitting.
It is important to note that spatial CV reduces the bias introduced by spatial autocorrelation but does not completely remove it. 
This is because there are still a few points in the test and training data which are still neighbors (@brenning_spatial_2012; see second row of \@ref(fig:partitioning)).

```{r partitioning, fig.cap="Spatial visualization of selected test and training observations for cross-validation of one repetition. Random (upper row) and spatial partitioning (lower row).", echo = FALSE}
knitr::include_graphics("figures/13_partitioning.png")
```

## Spatial CV with **mlr**
In R there are literally hundreds of packages available for statistical learning (e.g., have a look at the [CRAN task machine learning](https://CRAN.R-project.org/view=MachineLearning)).
In section \@ref(conventional-model) we used the **stats** package to fit a logistic regression using the `glm()` command.
`glm()` uses the common R modeling interface: 

1. specify the response and predictor variables via a formula object
2. build a model
3. make a prediction.

However, many packages come with their own or a modified statistical learning interface which is why users frequently have to spend a lot of time to figure out the specifics of each of these packages, how to carry out cross-validation and hyperparameter tuning or how to compare modeling results from different packages.
The **mlr** package acts as a meta- or umbrella-package providing a unified interface to the most popular statistical learning techniques available in R including classification, regression, survival analysis and clustering [@bischl_mlr:_2016].^[As pointed out in the beginning we will solely focus on supervised learning techniques in this chapter.]
The standardized **mlr** interface is based on so-called basic building blocks (Figure \@ref(fig:building-blocks)).

<!-- @Jakub: yes, I will ask if we me may use the figure -->
```{r building-blocks, echo=FALSE, fig.height=4, fig.width=4, fig.cap="Basic building blocks of the **mlr** package. Source: [openml.github.io](http://openml.github.io/articles/slides/useR2017_tutorial/slides_tutorial_files/ml_abstraction-crop.png)."}
knitr::include_graphics("figures/13_ml_abstraction_crop.png")
```

First, we need to create a **task** containing the data, specifically the response and predictor variables, for the model and the model type (such as regression or classification).
# Somehow the "or differently put" parts read not easily, maybe you can phrase it differently?
# A structure in the data is only learned for ML models as these are data driven. SL algs are assumption-driven and do not "learn" from the data in that sense.
# Maybe just write: "A learner defines the specific algorithm that is applied on the created task."
Secondly, a **learner** defines the specific algorithm that models the task data or differently put learns a structure inherent in the provided data.
Thirdly, we assess the predictive performance of the model, i.e. the model's ability to generalize the modeled relationship to new data via a repetitive **resampling** approach (see also section \@ref(intro-cv)).

### Generalized linear model {#glm}

To put the **mlr** approach into practice, we first create a **task** using our landslide data.
Since we have a binary response, which is in fact a two-category variable, we will create a classification task using `makeClassifTask()`.^[In the case of a regression problem, we would use `makeRegrTask()`.
Type `?makeClassifTask` to find out about all available modeling tasks.
]
First, we specify the data which will be used.
The `target` parameter expects the response variable and the `positive` parameter determines which of the two factor levels of the response variable indicate the landslide initiation point.
All other variables of the provided dataset will serve as predictors (check out with `getTaskFormula(task)`).
As we will perform a spatial CV later on, we need to specify the coordinates which will form the basis of the spatial partitioning (see section \@ref(intro-cv) and Figure \@ref(fig:partitioning)).
Also, we need to remove the coordinates from the dataset as we do not want to include these as predictors but explictily only for the spatial partitioning.
These have to be provided in a separate dataframe object in parameter `coordinates`. 

```{r}
library(mlr)
# separate data to be modeled and coordinates
coords = lsl[, c("x", "y")]
data = dplyr::select(lsl, -x, -y)
# create task
task = makeClassifTask(data = data, target = "lslpts",
                       positive = "TRUE", coordinates = coords)
```

`makeLearner()` determines the statistical learning method to use.
All classification **learners** start with `classif.` and all regression learners with `regr.` (see `?makeLearners` for more details). 
`listLearners()` helps to find out about all available learners and from which package **mlr** imports them. 
For a specific task, we can run:

<!-- no idea, why render_book() fails frequently because function listLearners() cannot be found...:
I also have this issue (RL - so not running and hardcoding result)-->

# maybe try to use a tibble instead of a dataframe. would save you the `head()` call and more informative print output

```{r, eval=FALSE}
lrns = listLearners(task)
dplyr::select(lrns, class, name, package) %>%
  head
#>                 class                         name package
#> 1    classif.binomial          Binomial Regression   stats
#> 2 classif.featureless       Featureless classifier     mlr
#> 3         classif.fnn     Fast k-Nearest Neighbour     FNN
#> 4         classif.knn           k-Nearest Neighbor   class
#> 5         classif.lda Linear Discriminant Analysis    MASS
#> 6      classif.logreg          Logistic Regression   stats
```

This yields all learners able to model two-class problems (landslide yes or no).
We opt for the binomial classification method from the **stats** package which we already have used in section \@ref(conventional-model) and is implemented as `classif.binomial` in **mlr**.
Additionally, we have to specify the link-function.
We choose the `logit` link which is also the default when using the `binomial` family in `glm` (run `binomial()` to verify).
`predict.type` determines the type of the prediction with
<!-- Setting it to `response` produces class labels as output, which would be in our case `TRUE` or `FALSE`. -->
 `prob` resulting in a predicted probability for landslide occurrence between 0 and 1.^[Note that this corresponds to `type = response` in `predict.glm`.]

```{r}
lrn = makeLearner(cl = "classif.binomial",
                  link = "logit",
                  predict.type = "prob",
                  fix.factors.prediction = TRUE)
# run the following lines to find out from which package the
# learner is taken and how to access the corresponding help 
# file(s)
# getLearnerPackages(learner)
# helpLearner(learner)
```

<!--
Having specified a learner and a task, we can train our model which basically executes the `glm()` command in the background for our task. 

```{r}
mod = train(learner = lrn, task = task)
mlr_fit = getLearnerModel(mod)
```

```{r, eval = FALSE, echo = FALSE}
getTaskFormula(task)
getTaskData(task)
getLearnerModel(mod)
mod$learner.model
```

`getLearnerModel()` extracts the used model which shows that **mlr** passed all specified parameters to the `glm` function in the background as also proved by following code:

```{r}
fit = glm(lslpts ~ ., family = binomial(link = "logit"), data = data)
identical(fit$coefficients, mlr_fit$coefficients)
```
-->

In the beginning, it might seem a bit tedious to learn the **mlr** interface for modeling.
But remember that one only has to learn one single interface to run 169 learners (**mlr** package version: `r packageVersion("mlr")`).
Further advantages are the easy parallelization of resampling techniques and the tuning of machine learning hyperparameters, also spatially, in an inner fold (see section \@ref(svm)).
Most importantly, (spatial) resampling in **mlr** is really easy, and only requires two more steps: specifying a resampling method and running it.
We will use a 100-repeated 5-fold spatial CV.
This ensures that a spatial partitioning with five partitions is chosen based on the provided coordinates in our `task` and that the partitioning is repeated 100 times.
# you could link to the arxiv preprint of my paper
Please note that package **sperrorest** initially implemented spatial cross-validation in R [@brenning_spatial_2012].
In the meantime, its functionality was integrated into the **mlr** package which is the reason why we are using **mlr** [@schratz_performance_nodate].^[The **caret** package is another umbrella-package [@kuhn_applied_2013] for streamlined modeling in R, however, so far it does not provide spatial CV which is why we refrain from using it for spatial data.]

```{r}
resampling = makeResampleDesc(method = "SpRepCV", folds = 5, 
                              reps = 100)
```

To execute the spatial resampling, we run `resample()` using the specified learner, task, resampling strategy and of course the performance measure, here the AUROC.
This takes a short while because we ask R to compute the AUROC from 500 models. 

# maybe explain the seed setting here (not everyone might be aware)

# I just thought it might be worth showing the differences between an error on the fold level and repetition level but aggregating to the rep levle is not a one-liner in mlr


```{r, eval=FALSE}
set.seed(012348)
sp_cv = mlr::resample(learner = lrn, task = task,
                      resampling = resampling, 
                      measures = mlr::auc)
```

<!-- sp_cv and conv_cv have been saved in spatialcv.Rdata. I needed to run the modeling outside of the book since knitr sets its own seed and I am not sure if this actually helps to make sure that the same partitions are used in the cv.
I really don't understand why I have to load spatialcv.Rdata here a third time...-->

```{r, echo=FALSE}
load("extdata/spatialcv.Rdata")
```

The output is a bias-reduced assessment of the model's predictive performance. 

```{r}
# summary statistics of the 500 models
summary(sp_cv$measures.test$auc)
# mean AUROC of the 500 models
mean(sp_cv$measures.test$auc)
```

To put it into perspective, we compare this result with that of a 100-repeated 5-fold non-spatial cross-validation (Figure \@ref(fig:boxplot-cv); the code for the non-spatial cross-validation is not shown here but will be explored in the exercise section).
As expected, the spatially cross-validated result yields lower AUROC values on average than the conventional cross-validation approach, underlining the over-optimistic predictive performance due to spatial autocorrelation of the latter.

```{r boxplot-cv, echo=FALSE, fig.width=6, fig.height=9, fig.cap="Boxplot showing the difference in AUROC values between spatial and conventional 100-repeated 5-fold cross-validation."}
# Visualization of non-spatial overfitting
boxplot(sp_cv$measures.test$auc,
        conv_cv$measures.test$auc,
        col = c("lightblue2", "mistyrose2"),
        names = c("spatial CV", "conventional CV"), 
        ylab = "AUROC")
```

### (Spatial) Tuning of machine-learning hyperparameters {#svm}
In the beginning we have already distinguished the field of statistics from the field of machine learning.
As a reminder we define machine learning here again with the words of [Jason Brownlee](https://machinelearningmastery.com/linear-regression-for-machine-learning/):

> Machine learning, more specifically the field of predictive modeling is primarily concerned with minimizing the error of a model or making the most accurate predictions possible, at the expense of explainability. In applied machine learning we will borrow, reuse and steal algorithms from many different fields, including statistics and use them towards these ends.

In the previous section (section \@ref(glm)) we have used a GLM for predicting landslide susceptibility, in this section we will introduce the support vector machine (SVM) for the same purpose.
It is beyond the scope of this book to explain exactly what a SVM is.
In short, SVMs are trying to find the best possible hyperplanes to separate classes (in a classification case).
Kernels with specific hyperparameters allow for non-linear boundaries between classes [@james_introduction_2013].
Hyperparameters should not be confused with coefficients of parametric models, which are sometimes also referred to as parameters.^[For a more detailed description of the difference between coefficients and hyperparameters, have a look at this [machine mastery blog](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/).]
# this is a neat and short differentation!
Coefficients can be estimated from the data while hyperparameters are set before the learning begins.
Optimal hyperparameters are usually determined within a defined range with the help of cross-validation methods.
This is called hyperparameter tuning.
Some SVM implementations come with an automated tuning which is usually based on random sampling (see upper row of Figure \@ref(fig:partitioning)) such as **kernlab**'s `ksvm()`.
This is useful in the case of non-spatial data but of less use in the case of spatial data where spatial tuning should be preferred.
# somethings missing here after "automated"
Therefore, we will make sure to replace automated by spatial hyperparameter tuning in the following.

Before defining the tuning, we have to set up the **mlr** building blocks for the SVM as introduced in the previous section (section \@ref(glm)), i.e. we define a task, a learner and a resampling strategy.
The task remains the same, hence we can use the one already defined in the previous section (section \@ref(glm)), namely `task`.
However, we have to define a new learner since we are going to use a SVM.
To find out which SVM functions are available we use again the `listLearners()` command.

```{r, eval=FALSE}
lrns = listLearners(task)
lrns[grep("svm", lrns$class), ]
dplyr::select(lrns, class, name, package) 
#>            class                                 name short.name package
#> 6   classif.ksvm              Support Vector Machines       ksvm kernlab
#> 9  classif.lssvm Least Squares Support Vector Machine      lssvm kernlab
#> 17   classif.svm     Support Vector Machines (libsvm)        svm   e1071
```

# reference not working
We will use `ksvm()` from the **kernlab** package [@Karatzoglou_kernlab_2004].
To allow for non-linear relationships we use the radial basis function (or Gaussian) kernel which is also the default of `ksvm()` and probably the most popular SVM kernel in general.

```{r, eval=FALSE}
lrn_ksvm = makeLearner("classif.ksvm",
                        predict.type = "prob",
                        kernel = "rbfdot")
```

Hence, the only thing left to do is to specify a resampling strategy.
Again we will use a 100-repeated 5-fold spatial CV.

# Instead of saying "outer resampling" we concluded to use "performance estimation level" and "tuning level" (inner) in our paper
# this is also what is shown in the nested CV figure so it would be more consistent 

```{r, eval = FALSE}
# outer resampling loop
outer = makeResampleDesc("SpRepCV", folds = 5, reps = 100)
```

So far, this is exactly the same as we have done when using the GLM (section \@ref(glm)), however, now we need to additionally tune the SVM hyperparameters.
Using the same data for the performance assessment and the tuning would potentially lead to overoptimistic results [@cawley_overfitting_2010].
To avoid this we will use a nested spatial CV.

```{r inner-outer, echo=FALSE, fig.cap="Visual representation of inner and outer folds in spatial and non-spatial cross-validation. Permission for reusing the figure was kindly granted by Patrick Schratz [@schratz_performance_nodate]."}
knitr::include_graphics("figures/13_cv.png")
```

This means that we split each fold again into five spatially disjoint subfolds which are used to determine the optimal hyperparameters (`inner` object in the code chunk below; see Figure \@ref(fig:inner-outer) for a visual representation).
To find the optimal hyperparameter combination we here fit 50 models in each of these subfolds with randomly selected hyperparameter values (`ctrl` object in the code chunk below).
Additionally, we restrict the randomly chosen values to a predefined tuning space (`ps` object).
The latter was chosen with values recommended in the literature [@schratz_performance_nodate].

<!--
Questions Pat:
- why not using e1071 svm -> inner hyperparameter tuning also possible I guess...
## Because kernlab has more kernel options. Other than that there is no argument
- explanation correct?
## If you mean the paragraph above, yes
- trafo-function?
## is just a different approach of writing the limits. You could also directly write 2^{-15}. Makes it easier to see the limits at the first glance. Personal preference though
- 125,000 models
- mc.set.seed = TRUE -> make sure that the partitioning remains the same in each thread
- can I compare the mean AUROC of the GLM and the SVM when using the same seed? Or is seeding not strictly necessary? I mean, ok, the partitions vary a bit but overall...
-->

```{r, eval=FALSE}
# five spatially disjoint partitions
inner = makeResampleDesc("SpCV", iters = 5)
# use 50 randomly selected hyperparameters
ctrl = makeTuneControlRandom(maxit = 50)
# define the outer limits of the randomly selected hyperparameters
ps = makeParamSet(
  makeNumericParam("C", lower = -12, upper = 15, trafo = function(x) 2^x),
  makeNumericParam("sigma", lower = -15, upper = 6, trafo = function(x) 2^x)
  )
```

Finally, we modify our learner `lrn_ksvm` in accordance with all the characteristics defining the hyperparameter tuning through a wrapper function.

```{r, eval=FALSE}
wrapped_lrn_ksvm = makeTuneWrapper(learner = lrn_ksvm, 
                                   resampling = inner,
                                   par.set = ps,
                                   control = ctrl, 
                                   show.info = TRUE,
                                   measures = mlr::auc)
```

# I understand what you mean there but I think its confusing for the average reader. 
# Here my take: "Overall, this set up implies that we ask R to fit 250 models to find the best hyperparameters for each fold. Doing this 5 times (for each fold) sums up to 1250 (250 \* 5) models per repetition. Since we are doing 100 repetitions, we are fitting a total of 125000 models just to find the optimal hyperparameters for each respective fold. (Note that in this count we do not count the models fitted with the optimal hyperparameters at the performance estimation level, which are additionally 500 (5 folds * 100 reps))."" 
Overall, this set up implies that we ask R to fit 250 models to determine the optimal hyperparameters which are then used five times, once for each fold (see also Figure \@ref(fig:partitioning)), for the performance assessment in the first repetition of the outer resampling loop.
This amounts to 250 * 5 models for one repetition
Since we are requesting 100 repetitions this leads to a total of 125,000 models. 



This is computationally quite demanding even with the small dataset used here.
So before starting the actual resampling it would be wise to reduce runtime with the help of a parallelization approach. 
In general, multicore processing is easier on Unix-based systems than on Windows systems.
# "cloud development is done on linux servers" is somehow a strange read that I cannot relate really. Maybe sth like: "Parallelilaztion and cloud-computing are most often done on Linux operating systems nowadays. This has some reasons, one of them that directly affects us is that only on Linux systems we can set a parallel seed in R that makes the parallel processes reproducible [this is still an assumption, I will check on that!]"
In fact, cloud computing is usually done and developed on Linux servers.
Therefore, we will present how to do nested cross-validation using a parallelization approach working only under Unix-based systems.^[Note also that the `mc.set.seeds` parameter used later on is only available on Unix-based systems].

Windows users have at least four options:

1. Run the resampling without parallelizing it though this will take most likely more than half a day.
2. Install a virtual machine (e.g. [Oracle VirtualBox](https://www.virtualbox.org/)) to reproduce the parallelization code.
3. Connect remotely to a Linux-Server (which is what we have done).
Of course, this also implies that R and all related packages are installed on the server.
4. Install [Docker](https://docs.docker.com/docker-for-windows/) and run a container made of Linux image with a pre-installed R configuration. 

Before starting the parallelization, we make sure that the processing continues even if one of the models throws an error by setting `on.learner.error` to `warn`.
This is quite handy since we really want to avoid that the processing stops just because of one failed model after running a server at full capacity for several days.
To inspect the failed models once the processing is completed, we dump them.

```{r, eval=FALSE}
configureMlr(on.learner.error = "warn", on.error.dump = TRUE)
```

To start the parallelization, we set the `mode` to `multicore` which will use `mclapply()` in the background on a single machine.^[Check out `?parallelStart` for further modes and the **parallelMap** [github page](https://github.com/berndbischl/parallelMap) for more information on the unified interface to popular parallelization back-ends.
Note also that `mclapply()` only supports multicore processing on Unix-based systems.]
`level` defines the level where to enable the parallelization with `mlr.tuneParams` determining that the inner resampling fold should be parallelized, i.e. the hyperparameter tuning (see lower left part of Figure \@ref(fig:inner-outer); and have a look at `parallelMap::parallelGetRegisteredLevels()` for supported levels as well as at the **mlr** [parallelization tutorial](https://mlr-org.github.io/mlr-tutorial/release/html/parallelization/index.html#parallelization-levels)).
# no one ones and cares how much cores you had available there, so proably just add a note: "make sure to check your numbers of cores on the server before setting a number. You will most likely not be the only user." Der automatisierte call zu 1/2 of avail cores ist gut!
Probably we are not the only ones using the server, therefore we are friendly and will use only half of its cores which in our case corresponds to 24 (`cpus` parameter).
# no, the partitions are created before the parallelization by the normal set.seed() call. mc.set.seed makes sure that the randomly chosen hyperparameters for the tuning are reproducible. These will first set within the parallelization.
To make sure that the same partitions are used in each parallelized thread, we set `mc.set.seed` to `TRUE`.

```{r, eval=FALSE}
# parallelize the tuning, i.e. the inner fold
parallelStart(mode = "multicore", 
              level = "mlr.tuneParams", 
              # just use half of the available cores
              cpus = round(parallel::detectCores() / 2),
              mc.set.seed = TRUE) 
```

Finally, we are all set up for conducting the nested spatial CV.
Specifying the `resample()` parameters follows the exact same procedure as presented when using a GLM with the only difference that we additionally tell it to give back the hyperparameter tuning results (`extract` parameter).
# Add: This is important if we could like to do a follow-up analysis on the hyperparameter tuning
After the processing, it is good practice to explicitly stop the parallelization which is exactly what `parallelStop()` is doing.
We also want to save the resulting `result` object that contains all results to disk to not have to recompute everything (`saveRDS()`).

```{r, eval=FALSE}
set.seed(12345)
result = mlr::resample(learner = wrapped_lrn_ksvm, 
                       task = task,
                       resampling = outer,
                       extract = getTuneResult,
                       measures = mlr::auc)
# stop parallelization
parallelStop()
# save your result, e.g.:
# saveRDS(result, "svm_sp_sp_rbf_50it.rda")
```

```{r, include=FALSE}
result = readRDS("extdata/svm_sp_sp_rbf_50it.rda")
```

Running 25,000 models using 24 cores took around 37 minutes.
Note that runtime depends on many aspects: CPU speed, the selected algorithm, the selected number of cores and the dataset.
You might first run only repetition and check its runtime before scaling it up to 100 repetitions.

```{r}
# Exploring the results
# run time in minutes
round(result$runtime / 60, 2)
```

Even more important than the runtime is the final aggregated AUROC, i.e. the model's ability to discriminate the two classes. 

```{r}
# final aggregated AUROC 
result$aggr
# same as
mean(result$measures.test$auc)
```

It appears that the GLM (aggregated AUROC was `r sp_cv$aggr`) is slightly better than the SVM in this specific case.
However, using more than 50 iterations in the random search would probably yield hyperparameters that result in models with a better AUROC [@schratz_performance_nodate].
# Is the footnote here not a bit unrelated to the meaning of the sentence (increasing random search iterations)?
On the other hand, increasing the number of random search iterations would also increase the total number of models to be fitted which then leads to an increased runtime.^[Have also a look at @james_introduction_2013 for a short discussion on the simililarity of logistic regression and SVM.]

Finally, we can have a look at the estimated optimal hyperparameters for each fold at the performance estimation level.
Note that the AUROC shown here refers to the estimated value during tuning.
This estimate is not included in the calculation of the overall performance.
It is usually higher than the value received from the fold at the performance estimation level to which the hyperparameters have been applied to.

```{r}
# winning hyperparameters of tuning step, i.e. the best combination out of 50 *
# 5 models
result$extract[[1]]
# here one can observe that the AUROC of the tuning test data is usually higher
# than for the model in the outer fold
result$measures.test[1, ]
```` 

# maybe add a figure (boxplot) showing the differences between tuning and no tuning?

## Conclusions
Resampling methods are a crucial part of a modern data scientist's toolbox [@james_introduction_2013]. 
In this chapter we used cross-validation to assess a model's predictive performance.
Spatial data is statistically often not independent due to spatial autocorrelation, which violates a fundamental assumption of cross-validation.
Therefore, we introduced spatial CV, which reduces the bias introduced by spatial autocorrelation. 
The **mlr** package makes it easy to use (spatial) resampling techniques in combination with the most popular statistical learning techniques including, of course, linear regression, but also semi-parametric models (e.g., generalized additive models) and typical machine learning techniques such as random forests, support vector machines or boosted regression trees [@bischl_mlr:_2016;@schratz_performance_nodate].
Machine learning algorithms often require the tuning of so-called hyperparameters.
# I would leave the footnote out here or rephrase it. It somehow reads that computation times is not increased anymore when we do hyperparameter tuning
Finding an optimal set of hyperparameters requires the fitting of thousands of models which substantially increases computing time.^[Naturally, computation time additionally increases with the size of the input data.]
To reduce computing time, **mlr** makes parallelization easy through various supported methods.

# maybe directly link to the spatial data chapter? https://mlr-org.github.io/mlr/articles/tutorial/devel/handling_of_spatial_data.html
Finally, for more details please check out also the fantastic **mlr** online documentation:

- https://mlr-org.github.io/mlr-tutorial/
- https://github.com/mlr-org/mlr/wiki/user-2015-tutorial

## Acknowledgments
We dearly thank Patrick Schratz (University of Jena) for fruitful discussions on **mlr** and for providing code input.


## Exercises

1. Compute the terrain attributes slope, plan curvature, profile curvature and catchment area from `dem` (provided by `data("landslides", package = "RSAGA")`) with the help of R-GIS bridges (see Chapter \@ref(gis)), and extract the values from the corresponding output rasters to the `landslides` dataframe (`data(landslides, package = "RSAGA"`)).
Keep all landslide initiation points and 175 randomly selected non-landslide points (see section \@ref(case-landslide)).
1. Use the derived terrain attribute rasters in combination with a GLM to make a spatial prediction map similar to Figure \@ref(fig:lsl-susc).
1. Compute a 100-repeated 5-fold non-spatial cross-validation and spatial CV based on the GLM learner and compare the AUROC values from both resampling strategies with the help of boxplots (see Figure \@ref(fig:boxplot-cv)).
Hint: You need to specify a non-spatial task and a non-spatial resampling strategy.
Before running the spatial cross-validation for both tasks set a seed to make sure that both use the same partitions which in turn guarantees comparability.
1. Model landslide susceptibility using a quadratic discriminant analysis [QDA, @james_introduction_2013].
Assess the predictive performance (AUROC) of the QDA. 
What is the a difference between the spatially cross-validated mean AUROC value of the QDA and the GLM?
Hint: Before running the spatial cross-validation for both learners set a seed to make sure that both use the same partitions which in turn guarantees comparability.
1. Run the SVM without tuning the hyperparameters.
Use the `rbfdot` kernel with $\sigma$ = 1 and *C* = 1. 
Leaving the hyperparameters unspecified in **kernlab**'s `ksvm()` would otherwise initialize an automatic non-spatial hyperparameter tuning.
For a discussion on the need for (spatial) tuning of hyperparameters please refer to @schratz_performance_nodate.
1. Model landslide susceptibility with the help of **mlr** using a random forest model as implemented by the **ranger** package.
Apply a nested spatial CV.
Parallelize the tuning level.
Use a random search with 50 iterations to find the optimal hyperparameter combination (here: `mtry` and `num.trees`).
The tuning space limits are 1 and 4 for `mtry`, and 1 and 10,000 for `num.trees`.
Warning: This might take a long time.


<!--
hyperparameter tuning:
The training data is again partitioned into 5 folds but only once.
Now each fold is used once as a test set, and the remaining training data is used to find the optimal hyperparameter tuning via a random search with 50 (or whatever number) iterations -> 250 iterations to find the optimal hyperparameter combination. 
This combination serves as input for the model in the outer level.

Hyperparameters are always tuned in mlr in an inner loop (I suppose). 
But why do we need the inner tuning.
Well, otherwise we would tune our hyperparameters on the test set of the outer loop, and this is like taking a sneak preview.
-->
